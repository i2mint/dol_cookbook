{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demos a few things about [composing stores](https://github.com/i2mint/dol/discussions/25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching extension to specific value decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll look at the (common) problem where you have a folder with various extensions, \n",
    "each requiring a different value decoder. \n",
    "How can you make a single store that will automatically look at the extension and decide how to decode the bytes of the file accordingly?\n",
    "\n",
    "There are two answers to this. \n",
    "\n",
    "The first, is to use a composition of multiple stores, each focused on a particular extension.\n",
    "\n",
    "The second, is to use the `postget` argument of `dol.wrap_kvs`. \n",
    "\n",
    "The right solution depends on the context, but I'd start with the `postget` solution most of the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some test data we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save_here.json',\n",
       " 'nested.json',\n",
       " 'AI and Tempo Estimation.pdf',\n",
       " 'JAMMIN-GPT.pdf',\n",
       " 'simple.docx',\n",
       " 'Release Notes.docx',\n",
       " 'simple.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dol import Files\n",
    "from dol_cookbook import misc_files_path\n",
    "\n",
    "s = Files(misc_files_path)\n",
    "list(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a text store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simple.txt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dol import wrap_kvs, filt_iter, Pipe\n",
    "\n",
    "text_wrap = Pipe(\n",
    "    filt_iter(filt=lambda x: x.endswith('.txt')),\n",
    "    wrap_kvs(value_decoder=lambda obj: obj.decode('utf-8'))\n",
    ")\n",
    "\n",
    "text_store = text_wrap(s)\n",
    "list(text_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k='simple.txt'\n",
      "v='This is\\nJust some text\\nTo test things'\n"
     ]
    }
   ],
   "source": [
    "store = text_store\n",
    "\n",
    "k = next(iter(store))\n",
    "print(f\"{k=}\")\n",
    "v = store[k]\n",
    "print(f\"{v=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a json store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save_here.json', 'nested.json']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_wrap = Pipe(\n",
    "    filt_iter(filt=lambda x: x.endswith('.json')),\n",
    "    wrap_kvs(value_decoder=lambda obj: json.loads(obj))\n",
    ")\n",
    "\n",
    "json_store = json_wrap(s)\n",
    "list(json_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k='save_here.json'\n",
      "v={'the': 'dict', 'you': 'want', 'to': 'save'}\n"
     ]
    }
   ],
   "source": [
    "store = json_store\n",
    "\n",
    "k = next(iter(store))\n",
    "print(f\"{k=}\")\n",
    "v = store[k]\n",
    "print(f\"{v=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a pdf store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI and Tempo Estimation.pdf', 'JAMMIN-GPT.pdf']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfdol.base import bytes_to_pdf_text_pages\n",
    "\n",
    "pdf_wrap = Pipe(\n",
    "    filt_iter(filt=lambda x: x.endswith('.pdf')),\n",
    "    bytes_to_pdf_text_pages,\n",
    "    wrap_kvs(value_decoder='\\n\\n------------\\n\\n'.join),\n",
    ")\n",
    "\n",
    "pdf_store = pdf_wrap(s)\n",
    "list(pdf_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k='AI and Tempo Estimation.pdf'\n",
      "v=' 1 AI and Tempo Estimation: A Review Geoff Luck1 1 Centre of Excellence in Music, Mind, Body and Brain, Department of Music, Art and Culture Studies, University of Jyväskylä, Finland.  1geoff.luck@jyu.fi   Abstract The author’s goal in this paper is to explore how artificial intelligence (AI) has been utilized to inform our understanding of and ability to estimate at scale a critical aspect of musical creativity — musical tempo. The central importance of tempo to musical creativity can be seen in how it is used to express specific emotions (Eerola and Vuoskoski 2013), suggest particular musical styles (Li and Chan 2011), influence perception of expression (Webster and Weir 2005) and mediate the urge to move one’s body in time to the music (Burger et al. 2014). Traditional tempo estimation methods typically detect signal periodicities that reflect the underlying rhythmic structure of the music, often using some form of autocorrelation of the amplitude envelope (Lartillot and Toiviainen 2007). Recently, AI-based methods utilizing convolutional or recurrent neural networks (CNNs, RNNs) on spectral representations of the audio signal have enjoyed significant improvements in accuracy (Aarabi and Peeters 2022). Common AI-based techniques include those based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)), classification and statistical learning (e.g., support vector machines (SVM)), and artificial neural networks (ANNs) (e.g., self-organizing maps (SOMs), CNNs, RNNs, deep learning (DL)). The aim here is to provide an overview of some of the more common AI-based tempo estimation algorithms and to shine a light on notable benefits and potential drawbacks of each. Limitations of AI in this field in general are also considered, as is the capacity for such methods to account for idiosyncrasies inherent in tempo perception, i.e., how well AI-based approaches are able to ‘think and act like humans.’ Introduction Artificial Intelligence (AI) can be defined as the simulation of human intelligence in computer systems programmed to think and act like humans (Russell, Norvig and Crouch 2010). The rapid development of AI and related technologies has had profound effects on a range of everyday and high-level activities and industries. From finance to healthcare, shopping, and transport, AI and its constituent and associated methods have opened a Pandora’s box of opportunity across a growing array of societal structures, including commerce, government, and academia. In particular, the impact of AI on creative processes marks perhaps the most significant incursion yet into what we conceptualize as true human activity.  Music is one such creative activity, a universal found in some form in all known human cultures. And recent work has revealed the potential for AI to transform how we understand, evaluate, and model creative musical processes. From composition to performance, recommendation to transcription, AI has been shown to have a range of impacts on this most human of creative endeavors. The author’s goal in this paper is to explore how AI has been utilized to inform our understanding of, and our ability to estimate at scale, one particular, fundamental aspect of musical creativity — musical tempo. Musical Tempo Tempo, including its absence, is a fundamental characteristic of any piece of music. Typically indicated in beats per minute (bpm), tempo refers to the speed or pace a musical work is (or is intended to be) played at. The fundamental importance of musical tempo can be seen in how it is utilized by composers and performers to express specific emotions (Eerola and Vuoskoski 2013), to suggest particular musical styles (Li and Chan 2011), and to build and release tension (Goodchild, Gingras and McAdams 2016). From a listeners’ perspective, musical tempo can influence perception of expression (Webster and Weir 2005), level of arousal (Lundqvist, Carlsson, Hilmersson, and Juslin 2009), as well as the urge to move one’s body in time to the music (Burger et al. 2014).  Musical tempo can be understood as representing two distinct concepts: A physical concept referring to the number of events produced per minute or a psychological concept corresponding to the number of events perceived per minute. The latter can be conceptualized as the rate at which a typical listener would tap or move along to a piece of music (Drake, Gros, and Penel 1999; Sachs 1953). The majority of scientific work focuses on perceptual tempo. Indeed, in the field of music information retrieval (MIR), tempo appears to be implicitly understood as the rate at which a listener would tap along to the music (Fraisse 1982). While there exists considerable individual variation in how listeners define the exact tempo of a piece of music (e.g., at which beat level or octave they tap along to), reliable estimation of perceived tempo across large corpora of music remains a key goal in MIR (Böck 2010). This is because tempo plays a crucial role in a range of applied MIR-related pursuits, including music classification (Nieto 2020), genre recognition (Tzanetakis and Cook 2002), emotion analysis (Cambouropoulos 2000), algorithmic generation (Mauch, Durieux, Müller and Riedl 2015), transcription (Boeck and Widmer 2014), sound source separation (Uhlich, Kim and Lee 2017), and recommendation (Zhang et al. 2018). The significance of tempo in the MIR community is further emphasized in the recurring competitions held between tempo extraction algorithms over the past two decades (Downie 2008).  Performance of an algorithm is typically assessed according to standardized metrics such as Accuracy0 (Acc0), Accuracy1 (Acc1), Accuracy2 (Acc2), and, where relevant, ClassAccuracy. Acc0 evaluates the number of (rounded) estimated tempo values that are identical to an annotated ground truth; Acc1 evaluates if estimated tempo lies within \\n\\n------------\\n\\n 2 +/- 4% of the annotated ground truth; Acc2 is the same as Acc1 but considers octave errors – confusing the actual tempo to its rhythmic counterparts – as correct; ClassAccuracy evaluates ability to correctly classify tempo from a range of classes where a classification approach is implemented. In terms of frameworks utilized, tempo estimation methods can be roughly divided into traditional approaches and more recent, AI-based methods. Traditional tempo estimation methods typically detect signal periodicities that reflect the underlying rhythmic structure of the music (Grzywczak and Gwardys 2014), often using some form of autocorrelation of the amplitude envelope (Toiviainen and Lartillot 2007). Examples of autocorrelation-based tempo estimation algorithms include the Miningsuite (Lartillot 2019) and marsyas (Tzanetakis and Percival 2013). The former enjoys widespread use in areas such as music psychology, the latter in the field of MIR. Both have achieved good results in tempo detection tasks and have been utilized in distinct areas of music information research, including both industry and academia. An alternative to these algorithms is to crawl tempo metadata via Spotify\\'s API. Although Spotify uses a proprietary algorithm not available to the public, it has nonetheless grown to be an industry standard with many uses also in academia. An open-source alternative is librosa (McFee et al. 2015), a Python package that can be used to extract temporal and spectral features from audio. Accuracy of these different possibilities varies, and none should be considered the de facto standard. More recently, AI-based methods utilizing convolutional neural networks (CNNs) or recurrent neural networks (RNNs) on spectral representations of the audio signal have enjoyed significant improvements in accuracy over their traditional counterparts (Aarabi and Peeters 2022). Examples of models employing convolutional and recurrent neural networks include Schr (Schreiber and Müller 2018) and böck (Böck, Krebs and Widmer 2015), respectively. Other AI-based techniques include those based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)), classification and statistical learning (e.g., support vector machines (SVMs)), and artificial neural networks (ANNs) (e.g., self-organizing maps (SOMs), CNNs, RNNs, DL). This is neither an exhaustive list nor the sole framework within which to categorize these approaches. It does, however incorporate many of the most common methods. Related to tempo estimation is recognition of rhythm patterns. The latter is really an extension of the former but has received less attention in the literature compared to tempo recognition. This is likely due to the difficulty in creating datasets that are annotated for rhythm patterns, since distinguishing between similar patterns remains a difficult task. As with tempo estimation, early systems for recognizing rhythm patterns used hand-crafted signal processing and statistical models, while more recent systems have used ML and DL techniques. Examples of these include systems that utilise a beat spectrum (Foote 2002), beat histogram (Tzanetakis 2002), harmonic analysis (Peeters 2011), and scale transform (Holzapfel 2011). The latter approach was extended by Marchand and Peeters (2014, 2016) by combining it with the modulation spectrum and adding correlation coefficients between frequency bands. What follows is an overview of some principal applications of these approaches to modeling and estimating perceived musical tempo and related tasks such as rhythm recognition and beat tracking. The object is to describe in general terms – and examine the level of accuracy achieved by – AI-based tempo estimation algorithms, as well as to shine a light on notable benefits and potential drawbacks of different approaches. Limitations of the use of AI in this field in general are also considered. Attention is paid to how well AI-based methods account for idiosyncrasies inherent in tempo perception, i.e., how well they are able to ‘think and act like humans.’ Gaps in the literature and future directions for research are also highlighted. AI and Musical Tempo 1. Reducing Octave Errors Traditional tempo estimation typically starts with detecting rhythmically related events and then estimating tempo by finding the dominant periodicity of the onsets related to the beat positions (Hainsworth 2004). Various techniques have been used for the latter aspect, including autocorrelation (Percival and Tzanetakis 2014), comb filters (Scheirer 1998), dynamic programming (Ellis 2007), Hidden Markov Models (Klapuri, Eronen and Astola 2006), and source separation (Gkiokas, Katsouros, Carayannis and Stafylakis 2012). Despite sizeable performance increases over the years, most of these approaches still suffer from octave errors. Many attempts have been made to reduce such errors by using techniques such as Gaussian Mixture Models (GMM) (Peeters and Flocon-Cholet 2012), neural networks (Böck, Krebs and Widmer 2015), k-nearest neighbour classification (k-NN) (Wu and Jang 2014), genre classification (Hörschläger, Vogl, Böck and Knees 2015), and Support Vector machines (SVM) (Percival and Tzanetakis 2014). Such methods have often been applied as a separate stage in a multi-stage approach. For instance, some authors have introduced a machine learning (ML)-driven classification step to help guide the algorithm to the correct tempo octave. Chen, Cremer, Lee, DiMaria, and Wu (2009) took a novel approach to this by using musical descriptors related to perceived mood to train a statistical model of perceived tempo classes in a unique 299-track dataset. Using an SVM and ground truth ratings as input, each track was pre-classified as being either 1) very slow, 2) somewhat slow, 3) somewhat fast, 4) very fast. The logic was that this first-stage classification could then be used to improve the accuracy of any conventional tempo estimation algorithm. To illustrate this, they tested the effect of their first-stage octave classifier in combination with a range of existing algorithms on the ISMIR04 and MIREX06 datasets. The algorithms selected were the 11 submitted to the ISMIR04 tempo estimation competition plus a new algorithm developed by the authors. Results revealed that the authors’ octave-corrected approach indeed led to a significant reduction in octave errors with these algorithms. In fact, performance across all algorithms improved by an average of 45% and 65.5% for the ISMIR04 and MIREX06 datasets, respectively. It’s worth emphasizing again that there was no additional low-level analysis of temporal events or repetition rates in the audio signal of each track: The corrective procedure was thus entirely independent of tempo-specific \\n\\n------------\\n\\n 3 information. This represents a major benefit of Chen et al.’s (2009) system. In addition, the fact that it can be used to improve any traditional tempo estimation algorithm makes it a very versatile approach. Nonetheless, even with the correction, only three of the algorithms tested achieved Acc1 performance above 50%. Wu (2015) also tackled the octave error problem by estimating tempo using a two-stage process, albeit in a slightly different fashion. First, the two most dominant tempi were estimated from a tempogram obtained from the short time Fourier transform of an onset detection function (ODF). Second, a k-NN or SVM classifier was used to discriminate the predominant tempo from the pair so identified. Wu (2015) evaluated this approach with the ISMIR04 Ballroom and Songs datasets. Results revealed a reduced rate of octave errors compared to previous approaches. Specifically, Wu achieved high levels of performance on Acc1 measures for both Ballroom (78.5%) and Songs (62.6%). In fact, both of these accuracy rates were superior to 7 tested alternatives – Klapuri (Gouyon et al 2006; Eronen and Klapuri 2010), Gkiokas-MA (Gkiokas, Katsourus and Carayiannis 2012), Gkiokas (Gkiokas et al 2012), Uhle (Gouyon et al 2006), Scheirer (Gouyon et al 2006; Scheirer 1998), Gainza-Hyb1 (Gainza and Coyle 2011), Gainza-Hyb2 (Gainza and Coyle 2011). For Acc2, performance was again superior to the same 7 alternatives for Ballroom (95%), but not for Songs (80.6%). The main benefit of Wu’s approach was generally superior performance compared to a range of previous approaches on the two databases selected for testing. The principal drawbacks were that 1) only a small number of datasets were tested, and 2) because of the relative homogeneity of the datasets utilized, it’s unclear how the system would perform on more genre-diverse collections. In a similar vein, Dutta (2018) framed octave error correction as a classification problem, and proposed a four-stage ‘tempo estimation plus octave correction’ architecture termed base+octv. Stage 1 involved detecting onsets; Stage 2 entailed beat period detection; Stage 3 involved Histogram building; and Stage 4 implemented octave classification via an SVM. The SVM utilized 5 features and a non-linear kernel to classify tempo into one of 3 octave classes. Like similar approaches, the goal was to reduce octave errors. Acc1 performance on 6 classic datasets (Ballroom, Hainsworth, ACM_MIRUM, GTZAN, ISMIR04, SMC_MIRUM) was found to be superior to a range of existing approaches (Percival (Percival and Tzanetakis 2014), Klapuri (Klapuri, Eronen and Astola 2006), Gkiokas (Gkiokas et al. 2012), IBT (Oliveiraet al. 2010), qm_vamp (Davies and Plumbley 2007), Scheirer (Scheirer 1998)), while Acc2 was found to be on-par with though not better than the state-of-the-art. Specifically, Acc1 performance was 69.6% (slightly better than the state of the art), while Acc2 performance was 91.2% (on-par with but not better than the state of the art). Dutta argues that their approach might classify unseen audio files better. However, they provide no evidence to support this assertion. The principal drawback is thus that this system was not tested on unseen data. The approaches outlined above, then, suggest that one way of reducing octave errors is to use classification techniques prior to or after principal tempo estimation to limit the range of possible tempos. The most versatile implementation of this approach seems to be that described by Chen et al (2009) since their classifier can apparently function in combination with any tempo estimation algorithm. Despite classification-based octave error reduction techniques improving performance of (mostly) traditional tempo estimation techniques, the field evolved with the development of neural network-based approaches and the framing of tempo estimation itself as a classification problem. 2. Tempo Estimation Itself as a Classification Problem More recently, the traditional-plus-octave-error-reduction approach has been largely supplanted by steadily-improving architectures based on neural networks. Here, the estimation of tempo itself is framed as a classification problem. Authors have made significant strides utilizing CNNs for this purpose. This work was born out of the use of CNNs in non-MIR fields, such as image classification. The widespread analysis of spectrograms in MIR research, and the visual nature of such time-frequency representations of music, led to standard computer vision CNNs being implemented to ‘see’ and classify events in spectrograms (e.g., Choi, Fazekas and Sandler 2016; Phan, Hertel, Maass and Mertins 2016; Han, Kim and Lee 2016). It seems noteworthy that most MIR DL scientists used spectrograms as input to their CNNs (Choi et al. 2016; Phan et al. 2016; Han et al. 2016; Pons, Lidy and Serra 2016; Schlüter and Böck 2014). Most even adopted stock rectangular filters for classification. This latter point highlights potential shortcomings of this approach: While images have spatial meaning, spectrograms’ dimensions represent time and frequency. Consequently, wider or higher filters might be capable of learning longer temporal dependencies in the audio domain or timbral features spread across a wider range of frequencies, respectively. The result would be musically- as opposed to visually-motivated filter shapes. Indeed, this is precisely what encouraged Pons and Serra (2017) to investigate whether MIR CNNs might benefit from a design oriented towards learning musical features rather than ‘seeing’ spectrograms, i.e., filter shapes adapted to musical concepts.  Pons and Serra (2017) proposed a novel design strategy for convolutional neural networks (CNNs) in music classification tasks, specifically spectrogram analysis. Their approach was to use different filter shapes adapted to fit musical concepts within the first layer. This is more expressive and certainly intuitive than the default small rectangular filters typically used. Pons & Serra (2017) developed a shallow approach in which two complimentary architectures designed to model onsets (O-net) or patterns (i.e., rhythm, tempo) (P-net) using shorter and longer filters, respectively, were implemented in parallel. By systematically manipulating combinations of the two with different parameter settings, the authors derived 8 different approaches. Performance of these approaches on the Ballroom dataset was then compared against 3 DL-based approaches – Time (Pons et al. 2016), Time-freq (Pons et al. 2016), Black-box (Pons et al. 2016) – and 1 non-DL approach – Marchand et al. (Marchand and Peeters, 2016). Results showed that Pons & Serra’s (2017) strategy was useful for fully exploiting the representational capacity of the first CNN layer when modelling music. Specifically, their best approach scored \\n\\n------------\\n\\n 4 second only to Marchand et al. in terms of tempo estimation accuracy. Building on Pons and Serra’s (2017) work, Schreiber and Müller (2019) further developed the use of CNNs for musical tempo estimation by exploiting the different semantics of spectrograms\\' time and frequency axes. Train/test datasets utilized were Ballroom (Schreiber and Müller 2018), EBall (Schreiber and Müller 2018; Gouyon et al. 2006; Marchand and Peeters 2016 b), GiantSteps Key (Knees et al. 2015), GiantSteps Tempo (Knees et al. 2015; Schreiber and Müller 2018 b), GTzan Key (Tzanetakis and Cook 2002; Kraft, Lerch and Zölzer 2013), GTzan Tempo (Tzanetakis and Cool 2002; Percival and Tzanetakis 2014), LMD Key (Raffel 2016; Schreiber 2017), LMD Tempo (Schreiber and Müller 2018; Raffel 2016), and MTG Tempo/MTG Key (Schreiber and Müller 2018; Faraldo et al. 2017). Employing a range of approaches, from shallow, domain-specific to deep variants with directional filters, they found that axis-aligned architectures performed on par with VGG-style networks developed for computer vision. At the same time, they were both less affected by confounding factors and required fewer model parameters. The adoption of neural networks by the MIR community, then, significantly increased the capabilities of tempo estimation algorithms. In particular, approaches that moved beyond ‘seeing’ spectrograms to applying directional filters based on musical concepts upped the overall level of performance in the field while at the same time simplifying the architectures. Still, these approaches largely only focused on estimating one musical characteristic – tempo – at a time. What came next were approaches able to estimate multiple characteristics – such as tempo, beat, and downbeat – simultaneously. 3. Multi-Task Methods for Simultaneous Estimation of Multiple Characteristics As we’ve seen so far, early tempo estimation methods based on the application of signal processing techniques such as autocorrelation analysis, comb filtering, and the discrete Fourier transform (DFT), to onset strength signals (OSS) extracted from the audio (e.g., Percival and Tzanetakis 2014; Böck, Krebs and Widmer 2015; Wu, Lee, Jan, Chang, Lu and Wang 2011) suffered from frequent octave confusion. More recently, deep neural networks (DNNs) used for direct tempo estimation, in which the task was framed as a classification problem (Schreiber and Müller 2018; Foroughmand and Peeters 2019) increased overall level of performance. More recently still, multi-task methods have been developed for joint estimation of multiple metrical elements, such as beat and downbeat (Goto 2001; Böck, Krebs and Widmer 2016) and beat, downbeat, and tempo (Böck, Davies and Knees 2019; Böck and Davies 2020). The overlap of the problems within these multi-task approaches have led them to achieve high levels of performance using innovations such as Long Short-Term Memory (LSTM) and Temporal Convolutional Networks (TCNs), as well as exploiting the benefits of training with data augmentation.  TCNs, which first appeared in the WaveNet generative audio model (van den Oord et al. 2016), were proposed by Davies and Böck (2019) as an alternative to a CNN approach using Bidirectional Long Short-Term Memory (BLSTM) for audio-based beat tracking. The authors observed that TCNs achieved state-of-the-art performance on a wide range of existing beat tracking datasets. They were also well suited to parallelization, allowing them to be trained efficiently even on very large datasets. Moreover, they required only a small number of weights. According to the authors, these attributes made TCNs a promising choice for audio-based beat tracking tasks. Böck, Davies and Knees (2019) built upon Davies and Böck’s (2019) beat-tracking system underpinned by temporal convolutional networks (TCNs). The authors proposed a multi-task learning system that simultaneously tracked beats and estimated tempo. As in Davies and Böck (2019) the system used TCNs, but here globally aggregated the skip connections, feeding them into a tempo classification layer. The multi-task nature of the system allowed it to exploit the mutual information of both tasks (by definition, the two tasks are highly interconnected) and improve one by learning only from the other. To assess the approach, a range of existing annotated datasets were used for training (Ballroom, Beatles, Hainsworth, Simac, SMC, HJDB) and testing (ACM Mirum, GiantSteps, GTZAN), and performance was evaluated against four reference systems (Gkiokas et al., Percival and Tzanetakis, Böck et al., Schreiber and Müller). Tempo estimation was evaluated with Acc1 and Acc2 measures. For both evaluation criteria, the multi-task approach achieved state-of-the-art performance at (both beat tracking and) tempo estimation. In particular, the system demonstrated improved performance on beat-tracking when trained with data that included tempo-only annotations. In other words, much like humans, the system learnt from information provided concerning a different but related task. However, no mention was made of tempo estimation performance when only beat-tracking data was included. Böck et al (2019) suggested that this approach may have a significant impact on beat tracking moving forward, as it allows for the use of alternative training data (global tempi) that are more prevalent and easier to annotate. The authors also discussed the computational benefits of the proposed approach, which include efficient training and reduced over-fitting. One potential drawback of Böck et al’s approach is that no mention is made of tempo estimation performance when only beat-tracking data is included. This isn’t strictly a negative aspect of the system, it’s just that no information is provided to clarify this. In terms of being human-like, the authors suggest that further research should be conducted on this \"compact\" deep model approach for generalization capabilities and re-use of information for end-users on unseen data. In a conceptual development of Davies and Böck (2019), Oyama, Ishizuka and Yoshii (2021) proposed a phase-aware method for jointly estimating beat and downbeat in popular music. They utilised a deep neural network (DNN) that estimated the beat phase at each frame instead of the beat presence probability. Their approach used all frames for training, not just a limited number of beat frames. The authors also modified the post-processing method for the estimated phase sequence. Different multi-task learning architectures for joint beat and downbeat detection were investigated, and the experimental results demonstrated the importance of phase modelling for stable beat and downbeat estimation. \\n\\n------------\\n\\n 5 A further multi-task development was proposed by Böck and Davies (2020).  They described a state-of-the-art DNN that simultaneously estimated tempo, beat location, and downbeat location. In particular, they used a data augmentation approach to expose their network to a wider range of information pertaining to these three aspects. Data augmentation, of course, is a major benefit of AI-oriented approaches in general, allowing a larger and more diverse dataset to be effectively created from a smaller more homogeneous one. The result of Böck and Davies’ approach was a performance increase of up to 6% over existing approaches.  The data augmentation argument above notwithstanding, large datasets of tempo-relevant annotations have facilitated development of so-called ‘data-driven’ tempo estimation in which ML algorithms learn from the annotated data. Initial developments in the field utilized algorithms based on approaches including bags of classifiers (Levy 2011), Gaussian Mixture Models (GMM) (Xiao et al. 2008; Peeters and Flocon-Cholet 2012), k-Nearest-Neighbors (k-NN) (Seyerlehner et al. 2007), Random Forests (Schreiber and Müller 2017), and SVMs (Chen et al. 2009; Gkiokas et al. 2012; Percival and Tzanetakis 2014). More recently, DL approaches have come to dominate the ML space.  An early DL-based tempo estimation system was that proposed by Böck, Krebs and Widmer (2015). This applied a bank of resonating comb filters to the output of an RNN to predict beat position then predict tempo from the estimated periodicity of the signal. Their approach did not use hand-crafted features. Instead, the authors used an intermediate beat-level representation of the signal as input to the comb filter bank. No complex post-processing was applied. Instead, the output was simply the highest resonator\\'s histogram peak. Böck et al’s approach achieved state-of-the-art performance on nine out of the ten datasets tested. An alternate method was proposed by Schreiber and Müller (2018) that swapped the comb filter for a mel-spectrogram-based approach again applied to a CNN. The latter approach framed tempo prediction as a classification task into tempo classes.  More recently, Foroughmand and Peeters (2019) developed a hybrid approach combining tempo- and genre-related information into a ‘hand-crafted-plus-data-driven’ approach they called Deep Rhythm (DR). DR represented a significant development because it simultaneously estimated tempo and classified rhythm patterns/genres. To do this, the authors proposed a new representation of the DNN input called harmonic constant-Q modulation (HCQM) that accounted for tempo frequencies in the harmonic series. DR considers how tempo and rhythm pattern interact to more accurately model the audio input. HCQM represented the harmonic series of tempo candidates in audio signals using a 4D-tensor, which was then used as input for a convolutional network to perform tempo and rhythm pattern estimation. In testing across multiple datasets, Foroughmand and Peeters observed incremental improvements in Acc1 for tempo estimation. At the same time, DR outperformed previous approaches on Acc1 and Acc2 measures, though only for Ballroom (ballroom music) and Giant-steps tempo (electronic music) test sets. This suggests that, in line with its name, DR performs best when rhythm is more clearly defined. Thus, DR offered incremental improvements in Acc1 for tempo estimation. Multi-task methods, in which multiple characteristics are estimated simultaneously really demonstrate the power of AI-based approaches in estimating temporal features of music. This is especially true in cases, such as the approach described by Böck et al (2019), in which training on one data type, e.g., tempo annotations, improves performance on a related but different task, e.g., beat tracking. One aspect of tempo estimation not discussed so far, and that has also benefited extensively from AI-driven approaches, is estimation of local vs global tempo. 4. Estimation of Local vs. Global Tempo Most of the literature on tempo estimation, and all that reviewed so far, focuses principally on global tempo, i.e., the average tempo of an entire piece of music. This is likely because a steady, largely isochronous beat is a characteristic of a significant proportion of recorded music. The result is that, in most musical styles, especially of the popular era, tempo remains relatively unchanged throughout a track. This is particularly true of electronic styles such as EDM in which the beat is entirely machine-driven. However, not all music exhibits such tempo(ral) isochrony. More temporally expressive styles, notably those in classical-related genres, can exhibit huge deviations from an ‘average’ or global tempo. For these styles, estimation of local tempo – the tempo at different moments in time or covering shorter epochs – is critical. Several authors have focused either on estimating local tempo only or joint estimation of both local and global tempo.   Schreiber, Zalkow and Müller (2020), for instance, modelled local tempo in a selection of classical music pieces. As noted above, tempo is known to fluctuate significantly in such music. They found that CNN-based approaches quite accurately captured local tempo even for such expressive classical styles as long as they were trained on the target genre. Importantly, they observed that their results were very dependent on the specific training-test split selected. In a more sophisticated approach, Schreiber & Müller (2018) trained a CNN to estimate both local and global tempo in what they termed a single-step approach. In a traditional setup, note onsets or beats are first identified, from which tempo is then estimated. Schreiber & Müller’s approach instead framed tempo estimation as a multi-class classification problem. This permitted the use of a single-step method. Their CNN, trained on a large dataset covering a wide range of genres and tempi, was able to estimate tempo based on less than 12 seconds of audio as input. The ability to estimate tempo on such a relatively short sample of music made their algorithm particularly suitable (with caveats) to estimate not just global but also local tempo.  Schreiber and Müller (2018) compared their approach to the böck (Böck, Krebs and Widmer 2015) and schr (Schreiber and Müller 2018) algorithms using a standard range of datasets: ACM Mirum (Peeters and Flocon-Cholet 2012), Ballroom (Gouyon et al 2006), GTzan (Tzanetakis and Cook 2002), Hainsworth (Hainsworth 2004), ISMIR04 (Gouyon et al 2006), GiantSteps Tempo (Knees et al. 2015), and SMC (Holzapfel et al. 2012), the union of which they termed Combined. Their new approach achieved the highest results in \\n\\n------------\\n\\n 6 terms of the strict metrics Acc0 (44.8%) and Acc1 (74.2%). For octave-error tolerance (Acc2), new (92.1%) was slightly outperformed by both böck (93.6%) and schr (92.2%), but all approaches performed very well. Results suggested that new was better than böck at correctly estimating tempo octave, while böck and schr were better if the metrical level was ignored. In terms of Acc1, new was significantly better than both böck and schr for the Ballroom (92.0%), GiantSteps (73.0%), and ACM Mirum (79.5%) datasets. The finding that böck and schr outperformed new on the more genre-diverse GTzan and Hainsworth datasets suggested that genre-wide training would improve the latter’s results on other datasets as well. So, Schreiber and Müller’s single-step approach compared very favorably to other state-of-the-art techniques in terms of global tempo estimation. However, because it also performed well at local tempo estimation, it was found to be useful for identifying and visualizing tempo drift in musical performances. This latter aspect is particularly useful for music analysis purposes. Further benefits included the fact that it did not rely on handcrafted features, instead being completely data-driven. Perhaps the most notable advantage of the system, though, was how well it dealt with tempo octave confusion. The significant reduction in such errors perhaps demonstrates how well it ‘thought and acted like a human.’ There were, nonetheless, a few drawbacks to the system. First, since Jazz, Classical, and Reggae genres were missing from the training data, it remains unclear how it would perform on more genre-diverse datasets. Second, the authors note that the network architecture could be improved by reducing the number of parameters, such as the use of shorter filters, dilated convolutions, residual connections, and a suitable replacement for the fully connected layers. The benefit of these changes, of course, would be to reduce the number of operations needed for training and estimation, making the approach more efficient in the process. In another multi-method approach, Istvanek (2021) compared conventional and state-of-the-art methods of beat tracking. While most tempo estimation work focuses on music with a broadly isochronous pulse, this study tackled the more complex task of tracking string quartet music. The conventional system tested for this purpose was the beat tracking module in the Python librosa library (McFee et al. 2015). Like most earlier approaches, this system tracks periodicity in the onset strength envelope via modification of spectral difference or spectral flux. Such frameworks are known to perform poorly with rapidly changing tempi typical of Western art music, although this problem can be mitigated to some extent using an adaptive window size based not on a fixed number of input values but a fixed number of inter-onset-intervals (Müller, Konz, Scharfstein, Ewert, and Clausen 2009).  The state-of-the-art method tested was the madmom Python module.i This uses a bidirectional RNN with Long LSTM cells, the latter allowing the network to store information relating to longer-term temporal structure. The idea was that this should permit more accurate detection of beat location. Probability estimation of beat location within frames was accomplished via a dynamic Bayesian network (DBN) approximated by an HMM.  In comparing the two approaches, Istvanek (2021) found that while librosa was best at estimating average global tempo, the RNN-based madmom module offered a better representation of the rhythmic structure and detected the highest number of individual beats. One question Istvanek (2021) raises is whether neural network-based systems should not be considered conventional today. Given their widespread, even dominant use, this seems like a valid question to ask. Also, musicological analysis requires minimum time spent on manual editing and ground truth annotation. Thus, accurate representation of rhythmic structure, especially number and position of beats, is crucial in any beat tracking system for this type of analysis. With increasing complexity and power, AI approaches have brought us a long way in tempo estimation in little more than a decade. In particular, methods of simultaneous estimation of both multiple characteristics and tempo across multiple time frames (local, global) have proved extremely successful. Nonetheless, architectures continue to increase in complexity, and the current state-of-the-art can provide tempo estimations bordering on perfect. In the final section, three particular state-of-the-art approaches are surveyed to give a flavor of where AI-driven tempo estimation stands today. 5. Increasing Complexity and State-of-the-Art de Souza, Moura and Briot (2021) compared tempo estimation performance of two systems based on artificial neural networks — an architecture utilizing a Bidirectional Recurrent Neural Network (B-RNN) that takes as its input the Mel spectrogram and which outputs the estimated tempo class, and Schreiber and Müller’s (2018) Convolutional Neural Network (CNN) architecture. Both networks were trained on the same extensive database (12,550 tracks), including percussion-only tracks, and compared both with each other and with Schreiber and Müller’s original(ly trained) algorithm. Results revealed that the B-RNN model performed on-par with (though in most cases did not outperform) the CNN-based approaches, but was particularly accurate for percussion-only tracks. This suggests that rhythmic elements play a mediating factor in the ability of neural networks to learn and predict musical tempo. As the authors note, further research on this is hampered by a lack of percussion-focused databases for testing and training. Song and Wang (2022) paired a hidden-Markov model (HMM) with a periodic recurrent neural network (PRNN) in an attempt to reduce computational complexity in a beat-tracking task. A significant such reduction was achieved by exploiting the frequency contents of the music signal via Fourier transform. Compared to previous implementations of artificial networks such as bidirectional recurrent neural networks (Bi-RNN) and temporal neural networks (TCN), neither of which can perceive the frequency of the musical beat, the HMM-with-PRNN approach achieved close to state-of-the-art performance but with significantly lower computational cost. Indeed, in additional to the high level of performance, this lower computational cost with state-of-the-art performance is clearly the main benefit of Song and Wang’s (2022) approach. As discussed above, Foroughmand and Peeters (2019) developed a hybrid approach combining tempo- and rhythm pattern-related information into a ‘hand-crafted-plus-data-driven’ approach they called Deep Rhythm (DR). Recently, Aarabi and Peeters (2022) extended Deep Rhythm to \\n\\n------------\\n\\n 7 simultaneously estimate both tempo and genre of a musical signal. This is possible because tempo and genre are highly correlated aspects of music. The system used a harmonic representation of rhythm as input to a CNN, processed through complex-valued convolutions to consider relationships between frequency bands. A multitask learning approach was then used to jointly estimate tempo and genre. Additionally, a second input branch was added to the system, using a mel-spectrogram input dedicated to timbre, which improved the performance of both tempo and genre estimation.  The authors’ network was trained on 8596 tracks from 3 datasets (Extended Ballroom (EBR) (Marchand and Peeters 2016), tempo MTG (tMTG) (Faraldo et al. 2017), tempo LMD (tLMD) (Raffel 2016)) and tested on a total of 3611 tracks across 7 independent datasets (ACM (Peeters and Flocon-Cholet 2012), ISMIR04 (Gouyon et al. 2006), Ballroom (BR) (Gouyon et al. 2006), Hainsworth (Hains) (Hainsworth 2004), GTzan (Marchand et al. 2015), SMC (Holzapfel et al. 2012), Giantsteps (GST) (Knees et al. 2015) as well as their union (Combined). Global tempo accuracy and octave errors were evaluated on Acc1 and Acc2 measures, respectively. Performance of several versions of the extended approach was compared against the original Deep Rhythm. A performance increase was observed cross all test datasets, with Acc1 of up to 97.7% and Acc2 of up to 99.4% (both on the GST dataset). Mean improvement across all datasets was 11.2% and 7% for Acc1 and Acc2, respectively. The biggest improvements were apparent for the SMC dataset (16.1% and 23.1% for Acc1 and Acc2, respectively), although performance on this dataset was still sub-par compared to all others. The chief benefits of Aarabi and Peeters’ (2022) DR approach was simultaneous estimation of both tempo and genre of music. While these two aspects are related, it can be seen that AI-based approaches are being developed which can capture increasingly disparate musical features. The overall improvement of several percentage points on most datasets was another clear win for DR. The only real drawback was the sub-par performance on the SMC dataset. Summary and Conclusions Instilling in machines the ability to ‘think and act like humans’ is a defining objective of AI. The author’s goal in this paper has been to survey how AI-based approaches have been utilized to inform understanding of and ability to estimate at scale perception of a fundamental aspect of music, namely its underlying tempo. Given individual variation in how humans define the exact tempo of a piece of music (e.g., at which beat level or octave they tap along to), one way of demonstrating human-like behavior might for a machine to mimic these idiosyncrasies in a convincing manner. Increasing temporal accuracy (Acc0 and Acc1 evaluation) and reducing octave errors (Acc2 evaluation) might thus be considered hallmarks of such behavior. Other indicators might include reduced complexity and increased efficiency. A range of tempo estimation system have been surveyed here that have progressively dealt with these idiosyncrasies and design characteristics. In the process, these algorithms have come to exhibit human-like qualities to an increasingly sophisticated degree. Building on earlier signal processing-oriented methods of tempo estimation based on detection of periodicities in the music, a range of AI-related techniques have been experimented with, including those based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)), classification and statistical learning (e.g., support vector machines (SVM)), and artificial neural networks (ANNs) (e.g., self-organizing maps (SOMs), CNNs, RNNs, deep learning (DL)). Along the way, specific variants of some of these approaches have been developed. These continually improving approaches have, one might argue, steadily encroached upon what is widely regarded a unique human capacity. So how much more accurate can tempo estimation get? In 2019, Schreiber, Urbano and Müller posed the question of whether we’re done yet with tempo estimation. Given the recent, near-perfect results achieved by the likes of Aarabi and Peeters (2022), for instance, which even then leave some small room for improvement, one might argue that the answer to that question is ‘No’. Will we ever be? In light of the myriad peculiarities of the human condition, and despite the attraction and unarguable utility of flawless imitation of human intelligence by a machine, even an ‘intelligent’ one, perfect estimation of perceptual tempo on a genuinely diverse range of music seems likely to remain forever just beyond reach. References Aarabi, H. F., and G. Peeters. 2022. “Extending deep rhythm for tempo and genre estimation using complex convolutions, multitask learning and multi-input network.” Journal of Creative Music Systems. Burger, B., M. R. Thompson, G. Luck, S. Saarikallio, and P. Toiviainen. 2014. “Hunting for the beat in the body: On period and phase locking in music-induced movement.” Frontiers in Human Neuroscience 8(903). Böck, S. 2010. “Onset, beat, and tempo detection with artificial neural networks.” Diploma thesis, Technical University of Munich, Germany, 2010. Böck, S., F. Krebs, and G. Widmer. 2015. “Accurate Tempo Estimation based on Recurrent Neural Networks and Resonating Comb Filters.” In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR), pp 625–631, Málaga, Spain, 2015. Böck, S., F. Krebs, and G. Widmer. 2016. “Joint beat and downbeat tracking with recurrent neural networks,” in International Society for Music Information Retrieval Conference (ISMIR), 2016, pp. 255–261. Böck, S., and M. E. P. Davies. 2020. “Deconstruct, analyse, reconstruct: How to improve tempo, beat, and downbeat estimation.” In Proc. of the 21st Int. Society for Music Information Retrieval Conf., Montréal, Canada, 2020. Böck, S., M. E. P. Davies, and P. Knees. 2019. “Multi-task learning of tempo and beat: Learning one to improve the other.” 20th International Society for Music Information Retrieval Conference, Delft, The Netherlands, 2019. Chen, C.-W., Cremer, M., Lee, K., DiMaria, P., and Wu, H.-H. (2009). Improving perceived tempo estimation by statistical modeling of higher-level musical descriptors. In Audio Engineering Society Convention 126. Audio Engineering Society.  Choi, K., G. Fazekas, and M. Sandler. 2016. “Automatic tagging using deep convolutional neural networks,” In 17th International Society for Music Information Retrieval Conference (ISMIR). Davies, M. E. P., and M.D. Plumbley. 2007. “Context-dependent beat tracking of musical audio.” IEEE Trans. Audio, Speech, Lang. Process., vol. 15(3), pp. 1009–1020. Davies, M. E. P., and S. Böck. 2019. “Temporal convolutional networks for musical audio beat tracking.” In Proc. of the 27th European Signal Processing Conf. (EU- SIPCO), 2019. Drake, C., L. Gros, and A. Penel. 1999. “How fast is that music? The relation between physical and perceived tempo.” In S. W. Yi (Ed.), Music, mind, and science (pp. 190–203), Seoul, Korea: Seoul National University. Dutta, O. 2018. “Tempo octave correction using multiclass support vector machine.” Proceedings of the 2nd International Conference on Inventive Communication and Computational Technologies (ICICCT 2018). \\n\\n------------\\n\\n 8 Eerola, T., and J. K. Vuoskoski. 2013. “A review of music and emotion studies: Approaches, emotion models, and stimuli.” Music Perception 30(3):307–340. Ellis, D. P. 2007. “Beat tracking by dynamic programming.” J. New Music Res., 36(1), pp. 51–60.  Eppler, A., A. Mäncchen, J. Abesser, C. Weiss, and K. Frieler. 2014. “Automatic style classification of jazz recordings with respect to rhythm, tempo, and tonality.” Proceedings of the 9th Conference on Interdisciplinary Musicology – CIM14. Berlin, Germany 2014. Eronen, A. J., and A. P. Klapuri. 2010. “Music Tempo Estimation With k-NN Regression.” IEEE Transactions on Speech and Audio Processing, 18 (1). Faraldo, A., Jorda, S., and Herrera, P. 2017. “A multi-profile method for key estimation in edm.” Proceedings of the AES International Conference on Semantic Audio. Erlangen, Germany: Audio Engineering Society.  Foote, J., M. L. Cooper, and U. Nam. 2002. “Audio retrieval by rhythmic similarity.” In Proc. of ISMIR (International Society for Music Information Retrieval), Paris, France, 2002. Foroughmand, H. and G. Peeters. (2019). “Deep-rhythm for tempo estimation and rhythm pattern recognition.” In Proc. of ISMIR (International Society for Music Information Retrieval), Delft, The Netherlands. Gainza, M., and E. Coyle. 2011. “Tempo detection using a hybrid multiband approach.” IEEE Transactions on Speech and Audio Processing, 19(1), 191-196. Gkiokas, A., V. Katsouros, and G. Carayannis. (2012). Reducing tempo octave errors by periodicity vector coding and svm learning. In Proc. of ISMIR (International Society for Music Information Retrieval), Porto, Portugal.  Gkiokas, A., V. Katsouros, G. Carayannis, and T. Stafylakis. 2012. “Music tempo estimation and beat tracking by applying source separation and metrical relations.” in Proc. Int. Conf. Acoust., Speech, Signal Process., pp. 421-424. Goodchild, M., B. Gingras, and S. McAdams. 2016. “Analysis, performance, and tension perception of an unmeasured prelude for harpsichord.” Music Perception 34(1):1–20. Goto, M. 2001. “An audio-based real-time beat tracking system for music with or without drum-sounds.” Journal of New Music Research,  pp. 159–171. Gouyon, F., A. P. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis, C. Uhle, and P. Cano. 2006. “An experimental comparison of audio tempo induction algorithms.“ IEEE Transactions on Audio, Speech, and Language Processing, 14(5):1832– 1844. 2006.  Grzywczak, D., and Gwardys, G. 2014. “Audio features in music information retrieval.” In D. Slezak, G. Schaefer, S. T. Vuong, and Y-S. Kim (Eds.) Proceedings of Active Media Technology, 10th International Conference, Warsaw, Poland, August 11–14, 2014. Hainsworth, S. W. 2004. “Techniques for the automated analysis of musical audio.” PhD thesis, University of Cambridge, UK, September 2004. Han, Y., J. Kim, and K. Lee. 2016. “Deep convolutional neural networks for predominant instrument recognition in polyphonic music.” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25(1), 208-221. Holzapfel, A., and Y. Stylianou. 2011. “Scale transform in rhythmic similarity of music.” IEEE Transactions on Audio, Speech and Language Processing, 19(1):176–185, 2011.  Holzapfel, A., M. E. P. Davies, J. R. Zapata, J. L. Oliveira, and F. Gouyon. 2012. “Selective sampling for beat tracking evaluation.” IEEE Transactions on Audio, Speech, and Language Processing, 20(9), 2539–2548, 2012.  Hörschläger, F., R. Vogl, S. Böck, and P. Knees. 2015. “Addressing tempo estimation octave errors in electronic music by incorporating style information extracted from Wikipedia”, In Proc. of the Sound and Music Computing Conference (SMC), Maynooth, Ireland, 2015. Istvanek, M. 2021. “The application of tempo calculation for musicological purposes.” Doctoral Degree Programme (2), FEEC BUT. DOI: 10.13164/eeict.2021.265 Klapuri, A., A. J. Eronen, and J. T. Astola. 2006. “Analysis of meter of acoustic music signals.” IEEE transactions on audio, speech, and language processing, 14(1), 342–355. Knees, P., Á. Faraldo, P. Herrera, R. Vogl, S. Böck, F. Hörschläger, and M. Le Goff. 2015. “Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections.” In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR), pp 364–370, Málaga, Spain, October 2015.  Kraft, S., A.Lerch, and U. Zölzer. 2013. “The tonalness spectrum: feature-based estimation of tonal components.” Proceedings of the 16th International Conference on Digital Audio Effects (DAFx), Maynooth, Ireland. Lartillot, O., and P. Toiviainen. 2007. “A Matlab toolbox for musical feature extraction from audio.” In Proceedings of the 10th International Conference on Digital Audio Effects, pp. 237-244. Levy, M. (2011). Improving perceptual tempo estimation with crowd-sourced annotations. In Proc. of ISMIR (International Society for Music Information Retrieval), Miami, Florida, USA.  Li, T. LH., and A. B. Chan. 2011. “Genre classification and the invariance of MFCC features to key and tempo.” In International Conference on MultiMedia Modeling pp. 317–327. Marchand, U., and G. Peeters. 2014. “The modulation scale spectrum and its application to rhythm-content description.” In Proc. of DAFx (International Conference on Digital Audio Effects), Erlangen, Germany, 2014.  Marchand, U., Fresnel, Q., and Peeters, G. (2015). “GTZAN-Rhythm: Extending the GTZAN Test-Set with Beat, Downbeat and Swing Annotations.” Late-Breaking Demo Session of the 16th International Society for Music Information Retrieval Conference, 2015.  Marchand, U., and G. Peeters. 2016. “Scale and shift invariant time/frequency representation using auditory statistics: Application to rhythm description.” In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2016.  Marchand, U., and G. Peeters. 2016 b. “The extended ballroom dataset.” Late Breaking Demo of the International Conference on Music Information Retrieval (ISMIR), New York, NY, USA. Müller M., V. Konz, A. Scharfstein, S. Ewert, and M. Clausen. 2009. “Towards automated extraction of tempo parameters from expressive music recordings.\" In: k. Hirata, G. Tzanetakis, K. Yoshii K (Eds.) ISMIR 2009, 10th international conference on music information retrieval. Oliveira, J. L., F. Gouyon, L. G. Martins, and L. P. Reis. 2010. “IBT: A real-time tempo and beat tracking system,” in Proc. Int. Soc. For Music Information Retrieval (ISMIR), pp. 291– 296, 2010. Oyama, T., R. Ishizuka, and K. Yoshii. 2021. “Phase-aware joint beat and downbeat estimation based on periodicity of metrical structure.” in Proc. of the 22nd Int. Society for Music Information Retrieval Conf., Online, 2021. Peeters. G. 2011. “Spectral and temporal periodicity representations of rhythm for the automatic classification of music audio signal.” IEEE Transactions on Audio, Speech and Language Processing, 19(5):1242– 1252, 2011.  Peeters, G., and J. Flocon-Cholet. 2012. “Perceptual tempo estimation using GMM-regression.” In Proceedings of the second international ACM workshop on Music information retrieval with user-centered and multimodal strategies (MIRUM), pp 45–50, New York, NY, USA, 2012. ACM.  Percival, G. and Tzanetakis, G. (2014). “Streamlined tempo estimation based on autocorrelation and cross-correlation with pulses.” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(12):1765–1776.  Phan, H., L. Hertel, M. Maass, and A. Mertins. 2016. “Robust audio event recognition with 1-max pooling convolutional neural networks.” arXiv preprint arXiv:1604.06338. Pons, J., T. Lidy, and X. Serra. 2016. “Experimenting with musically motivated convolutional neural networks.” In 14th International Workshop on Content- Based Multimedia Indexing (CBMI). IEEE. Pons, J., and X. Serra. 2017. “Designing efficient architectures for modelling temporal features with convolutional neural networks.” In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 2472-2476). IEEE. Raffel, C. (2016). Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching. PhD thesis, Columbia University.  Sachs, C. 1953. Rhythm and Tempo. London: Dent. Scheirer, E. D. 1998. “Tempo and beat analysis of acoustic musical signals.” J Acoust. Soc. Amer., 103(1), 588–601. \\u2028  Schlüter, J., and S. Böck. 2014. “Improved musical onset detection with convolutional neural networks.” In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. Schreiber. H. 2017. “CNN-based automatic musical key detection.” Music Information Retrieval Evaluation eXchange (MIREX), Suzhou, China. Schreiber, H. and Müller, M. (2017). A post-processing procedure for improving music tempo estimates using supervised learning. In Proc. of ISMIR (International Society for Music Information Retrieval), Suzhou, China.  \\n\\n------------\\n\\n 9 Schreiber, H., and M. Müller. 2018. “A single-step approach to musical tempo estimation using a convolutional neural network.” In 19th International Society for Music Information Retrieval Conference, Paris, France, 2018. Schreiber, H., and M. Müller. 2018 b. “A crowdsourced experiment for tempo estimation of electronic dance music.” Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR), Paris, France, September 2018. Schreiber, H., and M. Müller. 2019 a. “Musical tempo and key estimation using convolutional neural networks with directional filters.” arXiv preprint arXiv:1903.10839. Schreiber, H., J. Urbano, and M. Müller. 2019 b. Music tempo estimation: Are we done yet? Transactions of the International Society for Music Information Retrieval, 3(1), 111–125. Schreiber, H., F. Zalkow, and M. Müller. 2020. “Modeling and estimating local tempo: A case study on Chopin’s Mazurkas.” In Proc. of the 21st Int. Society for Music Information Retrieval Conf., Montréal, Canada, 2020.   Seyerlehner, K., Widmer, G., and Schnitzer, D. (2007). From rhythm patterns to perceived tempo. In Proc. of ISMIR (International Society for Music Information Retrieval), Vienna, Austria.  Tzanetakis, G., and P. Cook. 2002. “Musical genre classification of audio signals.” IEEE Transactions on Speech and Audio Processing, 10(5):293–302, 2002. van den Oord, A., S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. 2016. “Wavenet: A generative model for raw audio.” arXiv preprint arXiv:1609.03499. Webster, G. D. and C. G. Weir. 2005. “Emotional responses to music: Interactive effects of mode, texture, and tempo.” Motivation and Emotion 29(1):19-39. Wu, F-H. F. 2015. “Music tempo octave error reducing based on the statistics of tempogram.” 23rd Mediterranean Conference on Control and Automation (MED) June 16-19, 2015. Torremolinos, Spain. Wu, F-H. F. and J-S. R. Jang. 2014. “A supervised learning method for tempo estimation of musical audio”, In 22nd Mediterranean Conference of Control and Automation (MED), 2014, 599–604. IEEE, 2014. Wu, F.-H. F., T-C. Lee, J-S. R. Jang, K. K. Chang, C.-H. Lu, and W-N. Wang, 2011. “A two-fold dynamic programming approach to beat tracking for audio music with time-varying tempo.” in International Society for Music Information Retrieval Conference (ISMIR), 2011, pp. 191–196. Xiao, L., Tian, A., Li, W., and Zhou, J. (2008). “Using statistic model to capture the association between timbre and perceived tempo.” In Proc. of ISMIR (International Society for Music Information Retrieval), Philadelphia, PA, USA. Zapata, J. R., and Gómez, E. 2011. “Comparative evaluation and combination of audio tempo estimation approaches.” In Audio engineering society conference: 42nd international conference: Semantic audio. Audio Engineering Society.  i https://pypi.org/project/madmom/ '\n"
     ]
    }
   ],
   "source": [
    "store = pdf_store\n",
    "\n",
    "k = next(iter(store))\n",
    "print(f\"{k=}\")\n",
    "v = store[k]\n",
    "print(f\"{v=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a docx store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simple.docx', 'Release Notes.docx']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dol import Pipe\n",
    "from msword import bytes_to_doc, get_text_from_docx, only_files_with_msword_extension\n",
    "\n",
    "doc_wrap = Pipe(\n",
    "    only_files_with_msword_extension,\n",
    "    wrap_kvs(value_decoder=Pipe(bytes_to_doc, get_text_from_docx)),\n",
    ")\n",
    "\n",
    "doc_store = doc_wrap(s)\n",
    "list(doc_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k='simple.docx'\n",
      "v='Just a bit of text to show that is works. Another sentence.\\nThis is after a newline.\\n\\nThis is after two newlines.'\n"
     ]
    }
   ],
   "source": [
    "store = doc_store\n",
    "\n",
    "k = next(iter(store))\n",
    "print(f\"{k=}\")\n",
    "v = store[k]\n",
    "print(f\"{v=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a ChainMap from these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simple.docx',\n",
       " 'Release Notes.docx',\n",
       " 'AI and Tempo Estimation.pdf',\n",
       " 'JAMMIN-GPT.pdf',\n",
       " 'save_here.json',\n",
       " 'nested.json',\n",
       " 'simple.txt']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import ChainMap\n",
    "\n",
    "chained_store = ChainMap(text_store, json_store, pdf_store, doc_store)\n",
    "list(chained_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Just a bit of text to show that is works. Another sentence.\\nThis is after a newline.\\n\\nThis is after two newlines.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = chained_store['simple.docx']\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1 AI and Tempo Estimation: A Review Geoff Luck1 1 Centre of Excellence in Music, Mind, Body and Brain, Department of Music, Art and Culture Studies, University of Jyväskylä, Finland.  1geoff.luck@jyu.fi   Abstract The author’s goal in this paper is to explore how artificial intelligence (AI) has been utilized to inform our understanding of and ability to estimate at scale a critical aspect of musical creativity — musical tempo. The central importance of tempo to musical creativity can be seen in how it is used to express specific emotions (Eerola and Vuoskoski 2013), suggest particular musical styles (Li and Chan 2011), influence perception of expression (Webster and Weir 2005) and mediate the urge to move one’s body in time to the music (Burger et al. 2014). Traditional tempo estimation methods typically detect signal periodicities that reflect the underlying rhythmic structure of the music, often using some form of autocorrelation of the amplitude envelope (Lartillot and Toiviainen 2007). Recently, AI-based methods utilizing convolutional or recurrent neural networks (CNNs, RNNs) on spectral representations of the audio signal have enjoyed significant improvements in accuracy (Aarabi and Peeters 2022). Common AI-based techniques include those based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)), classification and statistical learning (e.g., support vector machines (SVM)), and artificial neural networks (ANNs) (e.g., self-organizing maps (SOMs), CNNs, RNNs, deep learning (DL)). The aim here is to provide an overview of some of the more common AI-based tempo estimation algorithms and to shine a light on notable benefits and potential drawbacks of each. Limitations of AI in this field in general are also considered, as is the capacity for such methods to account for idiosyncrasies inherent in tempo perception, i.e., how well AI-based approaches are able to ‘think and act like humans.’ Introduction Artificial Intelligence (AI) can be defined as the simulation of human intelligence in computer systems programmed to think and act like humans (Russell, Norvig and Crouch 2010). The rapid development of AI and related technologies has had profound effects on a range of everyday and high-level activities and industries. From finance to healthcare, shopping, and transport, AI and its constituent and associated methods have opened a Pandora’s box of opportunity across a growing array of societal structures, including commerce, government, and academia. In particular, the impact of AI on creative processes marks perhaps the most significant incursion yet into what we conceptualize as true human activity.  Music is one such creative activity, a universal found in some form in all known human cultures. And recent work has revealed the potential for AI to transform how we understand, evaluate, and model creative musical processes. From composition to performance, recommendation to transcription, AI has been shown to have a range of impacts on this most human of creative endeavors. The author’s goal in this paper is to explore how AI has been utilized to inform our understanding of, and our ability to estimate at scale, one particular, fundamental aspect of musical creativity — musical tempo. Musical Tempo Tempo, including its absence, is a fundamental characteristic of any piece of music. Typically indicated in beats per minute (bpm), tempo refers to the speed or pace a musical work is (or is intended to be) played at. The fundamental importance of musical tempo can be seen in how it is utilized by composers and performers to express specific emotions (Eerola and Vuoskoski 2013), to suggest particular musical styles (Li and Chan 2011), and to build and release tension (Goodchild, Gingras and McAdams 2016). From a listeners’ perspective, musical tempo can influence perception of expression (Webster and Weir 2005), level of arousal (Lundqvist, Carlsson, Hilmersson, and Juslin 2009), as well as the urge to move one’s body in time to the music (Burger et al. 2014).  Musical tempo can be understood as representing two distinct concepts: A physical concept referring to the number of events produced per minute or a psychological concept corresponding to the number of events perceived per minute. The latter can be conceptualized as the rate at which a typical listener would tap or move along to a piece of music (Drake, Gros, and Penel 1999; Sachs 1953). The majority of scientific work focuses on perceptual tempo. Indeed, in the field of music information retrieval (MIR), tempo appears to be implicitly understood as the rate at which a listener would tap along to the music (Fraisse 1982). While there exists considerable individual variation in how listeners define the exact tempo of a piece of music (e.g., at which beat level or octave they tap along to), reliable estimation of perceived tempo across large corpora of music remains a key goal in MIR (Böck 2010). This is because tempo plays a crucial role in a range of applied MIR-related pursuits, including music classification (Nieto 2020), genre recognition (Tzanetakis and Cook 2002), emotion analysis (Cambouropoulos 2000), algorithmic generation (Mauch, Durieux, Müller and Riedl 2015), transcription (Boeck and Widmer 2014), sound source separation (Uhlich, Kim and Lee 2017), and recommendation (Zhang et al. 2018). The significance of tempo in the MIR community is further emphasized in the recurring competitions held between tempo extraction algorithms over the past two decades (Downie 2008).  Performance of an algorithm is typically assessed according to standardized metrics such as Accuracy0 (Acc0), Accuracy1 (Acc1), Accuracy2 (Acc2), and, where relevant, ClassAccuracy. Acc0 evaluates the number of (rounded) estimated tempo values that are identical to an annotated ground truth; Acc1 evaluates if estimated tempo lies within \\n\\n------------\\n\\n 2 +/- 4% of the annotated ground truth; Acc2 is the same as Acc1 but considers octave errors – confusing the actual tempo to its rhythmic counterparts – as correct; ClassAccuracy evaluates ability to correctly classify tempo from a range of classes where a classification approach is implemented. In terms of frameworks utilized, tempo estimation methods can be roughly divided into traditional approaches and more recent, AI-based methods. Traditional tempo estimation methods typically detect signal periodicities that reflect the underlying rhythmic structure of the music (Grzywczak and Gwardys 2014), often using some form of autocorrelation of the amplitude envelope (Toiviainen and Lartillot 2007). Examples of autocorrelation-based tempo estimation algorithms include the Miningsuite (Lartillot 2019) and marsyas (Tzanetakis and Percival 2013). The former enjoys widespread use in areas such as music psychology, the latter in the field of MIR. Both have achieved good results in tempo detection tasks and have been utilized in distinct areas of music information research, including both industry and academia. An alternative to these algorithms is to crawl tempo metadata via Spotify\\'s API. Although Spotify uses a proprietary algorithm not available to the public, it has nonetheless grown to be an industry standard with many uses also in academia. An open-source alternative is librosa (McFee et al. 2015), a Python package that can be used to extract temporal and spectral features from audio. Accuracy of these different possibilities varies, and none should be considered the de facto standard. More recently, AI-based methods utilizing convolutional neural networks (CNNs) or recurrent neural networks (RNNs) on spectral representations of the audio signal have enjoyed significant improvements in accuracy over their traditional counterparts (Aarabi and Peeters 2022). Examples of models employing convolutional and recurrent neural networks include Schr (Schreiber and Müller 2018) and böck (Böck, Krebs and Widmer 2015), respectively. Other AI-based techniques include those based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)), classification and statistical learning (e.g., support vector machines (SVMs)), and artificial neural networks (ANNs) (e.g., self-organizing maps (SOMs), CNNs, RNNs, DL). This is neither an exhaustive list nor the sole framework within which to categorize these approaches. It does, however incorporate many of the most common methods. Related to tempo estimation is recognition of rhythm patterns. The latter is really an extension of the former but has received less attention in the literature compared to tempo recognition. This is likely due to the difficulty in creating datasets that are annotated for rhythm patterns, since distinguishing between similar patterns remains a difficult task. As with tempo estimation, early systems for recognizing rhythm patterns used hand-crafted signal processing and statistical models, while more recent systems have used ML and DL techniques. Examples of these include systems that utilise a beat spectrum (Foote 2002), beat histogram (Tzanetakis 2002), harmonic analysis (Peeters 2011), and scale transform (Holzapfel 2011). The latter approach was extended by Marchand and Peeters (2014, 2016) by combining it with the modulation spectrum and adding correlation coefficients between frequency bands. What follows is an overview of some principal applications of these approaches to modeling and estimating perceived musical tempo and related tasks such as rhythm recognition and beat tracking. The object is to describe in general terms – and examine the level of accuracy achieved by – AI-based tempo estimation algorithms, as well as to shine a light on notable benefits and potential drawbacks of different approaches. Limitations of the use of AI in this field in general are also considered. Attention is paid to how well AI-based methods account for idiosyncrasies inherent in tempo perception, i.e., how well they are able to ‘think and act like humans.’ Gaps in the literature and future directions for research are also highlighted. AI and Musical Tempo 1. Reducing Octave Errors Traditional tempo estimation typically starts with detecting rhythmically related events and then estimating tempo by finding the dominant periodicity of the onsets related to the beat positions (Hainsworth 2004). Various techniques have been used for the latter aspect, including autocorrelation (Percival and Tzanetakis 2014), comb filters (Scheirer 1998), dynamic programming (Ellis 2007), Hidden Markov Models (Klapuri, Eronen and Astola 2006), and source separation (Gkiokas, Katsouros, Carayannis and Stafylakis 2012). Despite sizeable performance increases over the years, most of these approaches still suffer from octave errors. Many attempts have been made to reduce such errors by using techniques such as Gaussian Mixture Models (GMM) (Peeters and Flocon-Cholet 2012), neural networks (Böck, Krebs and Widmer 2015), k-nearest neighbour classification (k-NN) (Wu and Jang 2014), genre classification (Hörschläger, Vogl, Böck and Knees 2015), and Support Vector machines (SVM) (Percival and Tzanetakis 2014). Such methods have often been applied as a separate stage in a multi-stage approach. For instance, some authors have introduced a machine learning (ML)-driven classification step to help guide the algorithm to the correct tempo octave. Chen, Cremer, Lee, DiMaria, and Wu (2009) took a novel approach to this by using musical descriptors related to perceived mood to train a statistical model of perceived tempo classes in a unique 299-track dataset. Using an SVM and ground truth ratings as input, each track was pre-classified as being either 1) very slow, 2) somewhat slow, 3) somewhat fast, 4) very fast. The logic was that this first-stage classification could then be used to improve the accuracy of any conventional tempo estimation algorithm. To illustrate this, they tested the effect of their first-stage octave classifier in combination with a range of existing algorithms on the ISMIR04 and MIREX06 datasets. The algorithms selected were the 11 submitted to the ISMIR04 tempo estimation competition plus a new algorithm developed by the authors. Results revealed that the authors’ octave-corrected approach indeed led to a significant reduction in octave errors with these algorithms. In fact, performance across all algorithms improved by an average of 45% and 65.5% for the ISMIR04 and MIREX06 datasets, respectively. It’s worth emphasizing again that there was no additional low-level analysis of temporal events or repetition rates in the audio signal of each track: The corrective procedure was thus entirely independent of tempo-specific \\n\\n------------\\n\\n 3 information. This represents a major benefit of Chen et al.’s (2009) system. In addition, the fact that it can be used to improve any traditional tempo estimation algorithm makes it a very versatile approach. Nonetheless, even with the correction, only three of the algorithms tested achieved Acc1 performance above 50%. Wu (2015) also tackled the octave error problem by estimating tempo using a two-stage process, albeit in a slightly different fashion. First, the two most dominant tempi were estimated from a tempogram obtained from the short time Fourier transform of an onset detection function (ODF). Second, a k-NN or SVM classifier was used to discriminate the predominant tempo from the pair so identified. Wu (2015) evaluated this approach with the ISMIR04 Ballroom and Songs datasets. Results revealed a reduced rate of octave errors compared to previous approaches. Specifically, Wu achieved high levels of performance on Acc1 measures for both Ballroom (78.5%) and Songs (62.6%). In fact, both of these accuracy rates were superior to 7 tested alternatives – Klapuri (Gouyon et al 2006; Eronen and Klapuri 2010), Gkiokas-MA (Gkiokas, Katsourus and Carayiannis 2012), Gkiokas (Gkiokas et al 2012), Uhle (Gouyon et al 2006), Scheirer (Gouyon et al 2006; Scheirer 1998), Gainza-Hyb1 (Gainza and Coyle 2011), Gainza-Hyb2 (Gainza and Coyle 2011). For Acc2, performance was again superior to the same 7 alternatives for Ballroom (95%), but not for Songs (80.6%). The main benefit of Wu’s approach was generally superior performance compared to a range of previous approaches on the two databases selected for testing. The principal drawbacks were that 1) only a small number of datasets were tested, and 2) because of the relative homogeneity of the datasets utilized, it’s unclear how the system would perform on more genre-diverse collections. In a similar vein, Dutta (2018) framed octave error correction as a classification problem, and proposed a four-stage ‘tempo estimation plus octave correction’ architecture termed base+octv. Stage 1 involved detecting onsets; Stage 2 entailed beat period detection; Stage 3 involved Histogram building; and Stage 4 implemented octave classification via an SVM. The SVM utilized 5 features and a non-linear kernel to classify tempo into one of 3 octave classes. Like similar approaches, the goal was to reduce octave errors. Acc1 performance on 6 classic datasets (Ballroom, Hainsworth, ACM_MIRUM, GTZAN, ISMIR04, SMC_MIRUM) was found to be superior to a range of existing approaches (Percival (Percival and Tzanetakis 2014), Klapuri (Klapuri, Eronen and Astola 2006), Gkiokas (Gkiokas et al. 2012), IBT (Oliveiraet al. 2010), qm_vamp (Davies and Plumbley 2007), Scheirer (Scheirer 1998)), while Acc2 was found to be on-par with though not better than the state-of-the-art. Specifically, Acc1 performance was 69.6% (slightly better than the state of the art), while Acc2 performance was 91.2% (on-par with but not better than the state of the art). Dutta argues that their approach might classify unseen audio files better. However, they provide no evidence to support this assertion. The principal drawback is thus that this system was not tested on unseen data. The approaches outlined above, then, suggest that one way of reducing octave errors is to use classification techniques prior to or after principal tempo estimation to limit the range of possible tempos. The most versatile implementation of this approach seems to be that described by Chen et al (2009) since their classifier can apparently function in combination with any tempo estimation algorithm. Despite classification-based octave error reduction techniques improving performance of (mostly) traditional tempo estimation techniques, the field evolved with the development of neural network-based approaches and the framing of tempo estimation itself as a classification problem. 2. Tempo Estimation Itself as a Classification Problem More recently, the traditional-plus-octave-error-reduction approach has been largely supplanted by steadily-improving architectures based on neural networks. Here, the estimation of tempo itself is framed as a classification problem. Authors have made significant strides utilizing CNNs for this purpose. This work was born out of the use of CNNs in non-MIR fields, such as image classification. The widespread analysis of spectrograms in MIR research, and the visual nature of such time-frequency representations of music, led to standard computer vision CNNs being implemented to ‘see’ and classify events in spectrograms (e.g., Choi, Fazekas and Sandler 2016; Phan, Hertel, Maass and Mertins 2016; Han, Kim and Lee 2016). It seems noteworthy that most MIR DL scientists used spectrograms as input to their CNNs (Choi et al. 2016; Phan et al. 2016; Han et al. 2016; Pons, Lidy and Serra 2016; Schlüter and Böck 2014). Most even adopted stock rectangular filters for classification. This latter point highlights potential shortcomings of this approach: While images have spatial meaning, spectrograms’ dimensions represent time and frequency. Consequently, wider or higher filters might be capable of learning longer temporal dependencies in the audio domain or timbral features spread across a wider range of frequencies, respectively. The result would be musically- as opposed to visually-motivated filter shapes. Indeed, this is precisely what encouraged Pons and Serra (2017) to investigate whether MIR CNNs might benefit from a design oriented towards learning musical features rather than ‘seeing’ spectrograms, i.e., filter shapes adapted to musical concepts.  Pons and Serra (2017) proposed a novel design strategy for convolutional neural networks (CNNs) in music classification tasks, specifically spectrogram analysis. Their approach was to use different filter shapes adapted to fit musical concepts within the first layer. This is more expressive and certainly intuitive than the default small rectangular filters typically used. Pons & Serra (2017) developed a shallow approach in which two complimentary architectures designed to model onsets (O-net) or patterns (i.e., rhythm, tempo) (P-net) using shorter and longer filters, respectively, were implemented in parallel. By systematically manipulating combinations of the two with different parameter settings, the authors derived 8 different approaches. Performance of these approaches on the Ballroom dataset was then compared against 3 DL-based approaches – Time (Pons et al. 2016), Time-freq (Pons et al. 2016), Black-box (Pons et al. 2016) – and 1 non-DL approach – Marchand et al. (Marchand and Peeters, 2016). Results showed that Pons & Serra’s (2017) strategy was useful for fully exploiting the representational capacity of the first CNN layer when modelling music. Specifically, their best approach scored \\n\\n------------\\n\\n 4 second only to Marchand et al. in terms of tempo estimation accuracy. Building on Pons and Serra’s (2017) work, Schreiber and Müller (2019) further developed the use of CNNs for musical tempo estimation by exploiting the different semantics of spectrograms\\' time and frequency axes. Train/test datasets utilized were Ballroom (Schreiber and Müller 2018), EBall (Schreiber and Müller 2018; Gouyon et al. 2006; Marchand and Peeters 2016 b), GiantSteps Key (Knees et al. 2015), GiantSteps Tempo (Knees et al. 2015; Schreiber and Müller 2018 b), GTzan Key (Tzanetakis and Cook 2002; Kraft, Lerch and Zölzer 2013), GTzan Tempo (Tzanetakis and Cool 2002; Percival and Tzanetakis 2014), LMD Key (Raffel 2016; Schreiber 2017), LMD Tempo (Schreiber and Müller 2018; Raffel 2016), and MTG Tempo/MTG Key (Schreiber and Müller 2018; Faraldo et al. 2017). Employing a range of approaches, from shallow, domain-specific to deep variants with directional filters, they found that axis-aligned architectures performed on par with VGG-style networks developed for computer vision. At the same time, they were both less affected by confounding factors and required fewer model parameters. The adoption of neural networks by the MIR community, then, significantly increased the capabilities of tempo estimation algorithms. In particular, approaches that moved beyond ‘seeing’ spectrograms to applying directional filters based on musical concepts upped the overall level of performance in the field while at the same time simplifying the architectures. Still, these approaches largely only focused on estimating one musical characteristic – tempo – at a time. What came next were approaches able to estimate multiple characteristics – such as tempo, beat, and downbeat – simultaneously. 3. Multi-Task Methods for Simultaneous Estimation of Multiple Characteristics As we’ve seen so far, early tempo estimation methods based on the application of signal processing techniques such as autocorrelation analysis, comb filtering, and the discrete Fourier transform (DFT), to onset strength signals (OSS) extracted from the audio (e.g., Percival and Tzanetakis 2014; Böck, Krebs and Widmer 2015; Wu, Lee, Jan, Chang, Lu and Wang 2011) suffered from frequent octave confusion. More recently, deep neural networks (DNNs) used for direct tempo estimation, in which the task was framed as a classification problem (Schreiber and Müller 2018; Foroughmand and Peeters 2019) increased overall level of performance. More recently still, multi-task methods have been developed for joint estimation of multiple metrical elements, such as beat and downbeat (Goto 2001; Böck, Krebs and Widmer 2016) and beat, downbeat, and tempo (Böck, Davies and Knees 2019; Böck and Davies 2020). The overlap of the problems within these multi-task approaches have led them to achieve high levels of performance using innovations such as Long Short-Term Memory (LSTM) and Temporal Convolutional Networks (TCNs), as well as exploiting the benefits of training with data augmentation.  TCNs, which first appeared in the WaveNet generative audio model (van den Oord et al. 2016), were proposed by Davies and Böck (2019) as an alternative to a CNN approach using Bidirectional Long Short-Term Memory (BLSTM) for audio-based beat tracking. The authors observed that TCNs achieved state-of-the-art performance on a wide range of existing beat tracking datasets. They were also well suited to parallelization, allowing them to be trained efficiently even on very large datasets. Moreover, they required only a small number of weights. According to the authors, these attributes made TCNs a promising choice for audio-based beat tracking tasks. Böck, Davies and Knees (2019) built upon Davies and Böck’s (2019) beat-tracking system underpinned by temporal convolutional networks (TCNs). The authors proposed a multi-task learning system that simultaneously tracked beats and estimated tempo. As in Davies and Böck (2019) the system used TCNs, but here globally aggregated the skip connections, feeding them into a tempo classification layer. The multi-task nature of the system allowed it to exploit the mutual information of both tasks (by definition, the two tasks are highly interconnected) and improve one by learning only from the other. To assess the approach, a range of existing annotated datasets were used for training (Ballroom, Beatles, Hainsworth, Simac, SMC, HJDB) and testing (ACM Mirum, GiantSteps, GTZAN), and performance was evaluated against four reference systems (Gkiokas et al., Percival and Tzanetakis, Böck et al., Schreiber and Müller). Tempo estimation was evaluated with Acc1 and Acc2 measures. For both evaluation criteria, the multi-task approach achieved state-of-the-art performance at (both beat tracking and) tempo estimation. In particular, the system demonstrated improved performance on beat-tracking when trained with data that included tempo-only annotations. In other words, much like humans, the system learnt from information provided concerning a different but related task. However, no mention was made of tempo estimation performance when only beat-tracking data was included. Böck et al (2019) suggested that this approach may have a significant impact on beat tracking moving forward, as it allows for the use of alternative training data (global tempi) that are more prevalent and easier to annotate. The authors also discussed the computational benefits of the proposed approach, which include efficient training and reduced over-fitting. One potential drawback of Böck et al’s approach is that no mention is made of tempo estimation performance when only beat-tracking data is included. This isn’t strictly a negative aspect of the system, it’s just that no information is provided to clarify this. In terms of being human-like, the authors suggest that further research should be conducted on this \"compact\" deep model approach for generalization capabilities and re-use of information for end-users on unseen data. In a conceptual development of Davies and Böck (2019), Oyama, Ishizuka and Yoshii (2021) proposed a phase-aware method for jointly estimating beat and downbeat in popular music. They utilised a deep neural network (DNN) that estimated the beat phase at each frame instead of the beat presence probability. Their approach used all frames for training, not just a limited number of beat frames. The authors also modified the post-processing method for the estimated phase sequence. Different multi-task learning architectures for joint beat and downbeat detection were investigated, and the experimental results demonstrated the importance of phase modelling for stable beat and downbeat estimation. \\n\\n------------\\n\\n 5 A further multi-task development was proposed by Böck and Davies (2020).  They described a state-of-the-art DNN that simultaneously estimated tempo, beat location, and downbeat location. In particular, they used a data augmentation approach to expose their network to a wider range of information pertaining to these three aspects. Data augmentation, of course, is a major benefit of AI-oriented approaches in general, allowing a larger and more diverse dataset to be effectively created from a smaller more homogeneous one. The result of Böck and Davies’ approach was a performance increase of up to 6% over existing approaches.  The data augmentation argument above notwithstanding, large datasets of tempo-relevant annotations have facilitated development of so-called ‘data-driven’ tempo estimation in which ML algorithms learn from the annotated data. Initial developments in the field utilized algorithms based on approaches including bags of classifiers (Levy 2011), Gaussian Mixture Models (GMM) (Xiao et al. 2008; Peeters and Flocon-Cholet 2012), k-Nearest-Neighbors (k-NN) (Seyerlehner et al. 2007), Random Forests (Schreiber and Müller 2017), and SVMs (Chen et al. 2009; Gkiokas et al. 2012; Percival and Tzanetakis 2014). More recently, DL approaches have come to dominate the ML space.  An early DL-based tempo estimation system was that proposed by Böck, Krebs and Widmer (2015). This applied a bank of resonating comb filters to the output of an RNN to predict beat position then predict tempo from the estimated periodicity of the signal. Their approach did not use hand-crafted features. Instead, the authors used an intermediate beat-level representation of the signal as input to the comb filter bank. No complex post-processing was applied. Instead, the output was simply the highest resonator\\'s histogram peak. Böck et al’s approach achieved state-of-the-art performance on nine out of the ten datasets tested. An alternate method was proposed by Schreiber and Müller (2018) that swapped the comb filter for a mel-spectrogram-based approach again applied to a CNN. The latter approach framed tempo prediction as a classification task into tempo classes.  More recently, Foroughmand and Peeters (2019) developed a hybrid approach combining tempo- and genre-related information into a ‘hand-crafted-plus-data-driven’ approach they called Deep Rhythm (DR). DR represented a significant development because it simultaneously estimated tempo and classified rhythm patterns/genres. To do this, the authors proposed a new representation of the DNN input called harmonic constant-Q modulation (HCQM) that accounted for tempo frequencies in the harmonic series. DR considers how tempo and rhythm pattern interact to more accurately model the audio input. HCQM represented the harmonic series of tempo candidates in audio signals using a 4D-tensor, which was then used as input for a convolutional network to perform tempo and rhythm pattern estimation. In testing across multiple datasets, Foroughmand and Peeters observed incremental improvements in Acc1 for tempo estimation. At the same time, DR outperformed previous approaches on Acc1 and Acc2 measures, though only for Ballroom (ballroom music) and Giant-steps tempo (electronic music) test sets. This suggests that, in line with its name, DR performs best when rhythm is more clearly defined. Thus, DR offered incremental improvements in Acc1 for tempo estimation. Multi-task methods, in which multiple characteristics are estimated simultaneously really demonstrate the power of AI-based approaches in estimating temporal features of music. This is especially true in cases, such as the approach described by Böck et al (2019), in which training on one data type, e.g., tempo annotations, improves performance on a related but different task, e.g., beat tracking. One aspect of tempo estimation not discussed so far, and that has also benefited extensively from AI-driven approaches, is estimation of local vs global tempo. 4. Estimation of Local vs. Global Tempo Most of the literature on tempo estimation, and all that reviewed so far, focuses principally on global tempo, i.e., the average tempo of an entire piece of music. This is likely because a steady, largely isochronous beat is a characteristic of a significant proportion of recorded music. The result is that, in most musical styles, especially of the popular era, tempo remains relatively unchanged throughout a track. This is particularly true of electronic styles such as EDM in which the beat is entirely machine-driven. However, not all music exhibits such tempo(ral) isochrony. More temporally expressive styles, notably those in classical-related genres, can exhibit huge deviations from an ‘average’ or global tempo. For these styles, estimation of local tempo – the tempo at different moments in time or covering shorter epochs – is critical. Several authors have focused either on estimating local tempo only or joint estimation of both local and global tempo.   Schreiber, Zalkow and Müller (2020), for instance, modelled local tempo in a selection of classical music pieces. As noted above, tempo is known to fluctuate significantly in such music. They found that CNN-based approaches quite accurately captured local tempo even for such expressive classical styles as long as they were trained on the target genre. Importantly, they observed that their results were very dependent on the specific training-test split selected. In a more sophisticated approach, Schreiber & Müller (2018) trained a CNN to estimate both local and global tempo in what they termed a single-step approach. In a traditional setup, note onsets or beats are first identified, from which tempo is then estimated. Schreiber & Müller’s approach instead framed tempo estimation as a multi-class classification problem. This permitted the use of a single-step method. Their CNN, trained on a large dataset covering a wide range of genres and tempi, was able to estimate tempo based on less than 12 seconds of audio as input. The ability to estimate tempo on such a relatively short sample of music made their algorithm particularly suitable (with caveats) to estimate not just global but also local tempo.  Schreiber and Müller (2018) compared their approach to the böck (Böck, Krebs and Widmer 2015) and schr (Schreiber and Müller 2018) algorithms using a standard range of datasets: ACM Mirum (Peeters and Flocon-Cholet 2012), Ballroom (Gouyon et al 2006), GTzan (Tzanetakis and Cook 2002), Hainsworth (Hainsworth 2004), ISMIR04 (Gouyon et al 2006), GiantSteps Tempo (Knees et al. 2015), and SMC (Holzapfel et al. 2012), the union of which they termed Combined. Their new approach achieved the highest results in \\n\\n------------\\n\\n 6 terms of the strict metrics Acc0 (44.8%) and Acc1 (74.2%). For octave-error tolerance (Acc2), new (92.1%) was slightly outperformed by both böck (93.6%) and schr (92.2%), but all approaches performed very well. Results suggested that new was better than böck at correctly estimating tempo octave, while böck and schr were better if the metrical level was ignored. In terms of Acc1, new was significantly better than both böck and schr for the Ballroom (92.0%), GiantSteps (73.0%), and ACM Mirum (79.5%) datasets. The finding that böck and schr outperformed new on the more genre-diverse GTzan and Hainsworth datasets suggested that genre-wide training would improve the latter’s results on other datasets as well. So, Schreiber and Müller’s single-step approach compared very favorably to other state-of-the-art techniques in terms of global tempo estimation. However, because it also performed well at local tempo estimation, it was found to be useful for identifying and visualizing tempo drift in musical performances. This latter aspect is particularly useful for music analysis purposes. Further benefits included the fact that it did not rely on handcrafted features, instead being completely data-driven. Perhaps the most notable advantage of the system, though, was how well it dealt with tempo octave confusion. The significant reduction in such errors perhaps demonstrates how well it ‘thought and acted like a human.’ There were, nonetheless, a few drawbacks to the system. First, since Jazz, Classical, and Reggae genres were missing from the training data, it remains unclear how it would perform on more genre-diverse datasets. Second, the authors note that the network architecture could be improved by reducing the number of parameters, such as the use of shorter filters, dilated convolutions, residual connections, and a suitable replacement for the fully connected layers. The benefit of these changes, of course, would be to reduce the number of operations needed for training and estimation, making the approach more efficient in the process. In another multi-method approach, Istvanek (2021) compared conventional and state-of-the-art methods of beat tracking. While most tempo estimation work focuses on music with a broadly isochronous pulse, this study tackled the more complex task of tracking string quartet music. The conventional system tested for this purpose was the beat tracking module in the Python librosa library (McFee et al. 2015). Like most earlier approaches, this system tracks periodicity in the onset strength envelope via modification of spectral difference or spectral flux. Such frameworks are known to perform poorly with rapidly changing tempi typical of Western art music, although this problem can be mitigated to some extent using an adaptive window size based not on a fixed number of input values but a fixed number of inter-onset-intervals (Müller, Konz, Scharfstein, Ewert, and Clausen 2009).  The state-of-the-art method tested was the madmom Python module.i This uses a bidirectional RNN with Long LSTM cells, the latter allowing the network to store information relating to longer-term temporal structure. The idea was that this should permit more accurate detection of beat location. Probability estimation of beat location within frames was accomplished via a dynamic Bayesian network (DBN) approximated by an HMM.  In comparing the two approaches, Istvanek (2021) found that while librosa was best at estimating average global tempo, the RNN-based madmom module offered a better representation of the rhythmic structure and detected the highest number of individual beats. One question Istvanek (2021) raises is whether neural network-based systems should not be considered conventional today. Given their widespread, even dominant use, this seems like a valid question to ask. Also, musicological analysis requires minimum time spent on manual editing and ground truth annotation. Thus, accurate representation of rhythmic structure, especially number and position of beats, is crucial in any beat tracking system for this type of analysis. With increasing complexity and power, AI approaches have brought us a long way in tempo estimation in little more than a decade. In particular, methods of simultaneous estimation of both multiple characteristics and tempo across multiple time frames (local, global) have proved extremely successful. Nonetheless, architectures continue to increase in complexity, and the current state-of-the-art can provide tempo estimations bordering on perfect. In the final section, three particular state-of-the-art approaches are surveyed to give a flavor of where AI-driven tempo estimation stands today. 5. Increasing Complexity and State-of-the-Art de Souza, Moura and Briot (2021) compared tempo estimation performance of two systems based on artificial neural networks — an architecture utilizing a Bidirectional Recurrent Neural Network (B-RNN) that takes as its input the Mel spectrogram and which outputs the estimated tempo class, and Schreiber and Müller’s (2018) Convolutional Neural Network (CNN) architecture. Both networks were trained on the same extensive database (12,550 tracks), including percussion-only tracks, and compared both with each other and with Schreiber and Müller’s original(ly trained) algorithm. Results revealed that the B-RNN model performed on-par with (though in most cases did not outperform) the CNN-based approaches, but was particularly accurate for percussion-only tracks. This suggests that rhythmic elements play a mediating factor in the ability of neural networks to learn and predict musical tempo. As the authors note, further research on this is hampered by a lack of percussion-focused databases for testing and training. Song and Wang (2022) paired a hidden-Markov model (HMM) with a periodic recurrent neural network (PRNN) in an attempt to reduce computational complexity in a beat-tracking task. A significant such reduction was achieved by exploiting the frequency contents of the music signal via Fourier transform. Compared to previous implementations of artificial networks such as bidirectional recurrent neural networks (Bi-RNN) and temporal neural networks (TCN), neither of which can perceive the frequency of the musical beat, the HMM-with-PRNN approach achieved close to state-of-the-art performance but with significantly lower computational cost. Indeed, in additional to the high level of performance, this lower computational cost with state-of-the-art performance is clearly the main benefit of Song and Wang’s (2022) approach. As discussed above, Foroughmand and Peeters (2019) developed a hybrid approach combining tempo- and rhythm pattern-related information into a ‘hand-crafted-plus-data-driven’ approach they called Deep Rhythm (DR). Recently, Aarabi and Peeters (2022) extended Deep Rhythm to \\n\\n------------\\n\\n 7 simultaneously estimate both tempo and genre of a musical signal. This is possible because tempo and genre are highly correlated aspects of music. The system used a harmonic representation of rhythm as input to a CNN, processed through complex-valued convolutions to consider relationships between frequency bands. A multitask learning approach was then used to jointly estimate tempo and genre. Additionally, a second input branch was added to the system, using a mel-spectrogram input dedicated to timbre, which improved the performance of both tempo and genre estimation.  The authors’ network was trained on 8596 tracks from 3 datasets (Extended Ballroom (EBR) (Marchand and Peeters 2016), tempo MTG (tMTG) (Faraldo et al. 2017), tempo LMD (tLMD) (Raffel 2016)) and tested on a total of 3611 tracks across 7 independent datasets (ACM (Peeters and Flocon-Cholet 2012), ISMIR04 (Gouyon et al. 2006), Ballroom (BR) (Gouyon et al. 2006), Hainsworth (Hains) (Hainsworth 2004), GTzan (Marchand et al. 2015), SMC (Holzapfel et al. 2012), Giantsteps (GST) (Knees et al. 2015) as well as their union (Combined). Global tempo accuracy and octave errors were evaluated on Acc1 and Acc2 measures, respectively. Performance of several versions of the extended approach was compared against the original Deep Rhythm. A performance increase was observed cross all test datasets, with Acc1 of up to 97.7% and Acc2 of up to 99.4% (both on the GST dataset). Mean improvement across all datasets was 11.2% and 7% for Acc1 and Acc2, respectively. The biggest improvements were apparent for the SMC dataset (16.1% and 23.1% for Acc1 and Acc2, respectively), although performance on this dataset was still sub-par compared to all others. The chief benefits of Aarabi and Peeters’ (2022) DR approach was simultaneous estimation of both tempo and genre of music. While these two aspects are related, it can be seen that AI-based approaches are being developed which can capture increasingly disparate musical features. The overall improvement of several percentage points on most datasets was another clear win for DR. The only real drawback was the sub-par performance on the SMC dataset. Summary and Conclusions Instilling in machines the ability to ‘think and act like humans’ is a defining objective of AI. The author’s goal in this paper has been to survey how AI-based approaches have been utilized to inform understanding of and ability to estimate at scale perception of a fundamental aspect of music, namely its underlying tempo. Given individual variation in how humans define the exact tempo of a piece of music (e.g., at which beat level or octave they tap along to), one way of demonstrating human-like behavior might for a machine to mimic these idiosyncrasies in a convincing manner. Increasing temporal accuracy (Acc0 and Acc1 evaluation) and reducing octave errors (Acc2 evaluation) might thus be considered hallmarks of such behavior. Other indicators might include reduced complexity and increased efficiency. A range of tempo estimation system have been surveyed here that have progressively dealt with these idiosyncrasies and design characteristics. In the process, these algorithms have come to exhibit human-like qualities to an increasingly sophisticated degree. Building on earlier signal processing-oriented methods of tempo estimation based on detection of periodicities in the music, a range of AI-related techniques have been experimented with, including those based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)), classification and statistical learning (e.g., support vector machines (SVM)), and artificial neural networks (ANNs) (e.g., self-organizing maps (SOMs), CNNs, RNNs, deep learning (DL)). Along the way, specific variants of some of these approaches have been developed. These continually improving approaches have, one might argue, steadily encroached upon what is widely regarded a unique human capacity. So how much more accurate can tempo estimation get? In 2019, Schreiber, Urbano and Müller posed the question of whether we’re done yet with tempo estimation. Given the recent, near-perfect results achieved by the likes of Aarabi and Peeters (2022), for instance, which even then leave some small room for improvement, one might argue that the answer to that question is ‘No’. Will we ever be? In light of the myriad peculiarities of the human condition, and despite the attraction and unarguable utility of flawless imitation of human intelligence by a machine, even an ‘intelligent’ one, perfect estimation of perceptual tempo on a genuinely diverse range of music seems likely to remain forever just beyond reach. References Aarabi, H. F., and G. Peeters. 2022. “Extending deep rhythm for tempo and genre estimation using complex convolutions, multitask learning and multi-input network.” Journal of Creative Music Systems. Burger, B., M. R. Thompson, G. Luck, S. Saarikallio, and P. Toiviainen. 2014. “Hunting for the beat in the body: On period and phase locking in music-induced movement.” Frontiers in Human Neuroscience 8(903). Böck, S. 2010. “Onset, beat, and tempo detection with artificial neural networks.” Diploma thesis, Technical University of Munich, Germany, 2010. Böck, S., F. Krebs, and G. Widmer. 2015. “Accurate Tempo Estimation based on Recurrent Neural Networks and Resonating Comb Filters.” In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR), pp 625–631, Málaga, Spain, 2015. Böck, S., F. Krebs, and G. Widmer. 2016. “Joint beat and downbeat tracking with recurrent neural networks,” in International Society for Music Information Retrieval Conference (ISMIR), 2016, pp. 255–261. Böck, S., and M. E. P. Davies. 2020. “Deconstruct, analyse, reconstruct: How to improve tempo, beat, and downbeat estimation.” In Proc. of the 21st Int. Society for Music Information Retrieval Conf., Montréal, Canada, 2020. Böck, S., M. E. P. Davies, and P. Knees. 2019. “Multi-task learning of tempo and beat: Learning one to improve the other.” 20th International Society for Music Information Retrieval Conference, Delft, The Netherlands, 2019. Chen, C.-W., Cremer, M., Lee, K., DiMaria, P., and Wu, H.-H. (2009). Improving perceived tempo estimation by statistical modeling of higher-level musical descriptors. In Audio Engineering Society Convention 126. Audio Engineering Society.  Choi, K., G. Fazekas, and M. Sandler. 2016. “Automatic tagging using deep convolutional neural networks,” In 17th International Society for Music Information Retrieval Conference (ISMIR). Davies, M. E. P., and M.D. Plumbley. 2007. “Context-dependent beat tracking of musical audio.” IEEE Trans. Audio, Speech, Lang. Process., vol. 15(3), pp. 1009–1020. Davies, M. E. P., and S. Böck. 2019. “Temporal convolutional networks for musical audio beat tracking.” In Proc. of the 27th European Signal Processing Conf. (EU- SIPCO), 2019. Drake, C., L. Gros, and A. Penel. 1999. “How fast is that music? The relation between physical and perceived tempo.” In S. W. Yi (Ed.), Music, mind, and science (pp. 190–203), Seoul, Korea: Seoul National University. Dutta, O. 2018. “Tempo octave correction using multiclass support vector machine.” Proceedings of the 2nd International Conference on Inventive Communication and Computational Technologies (ICICCT 2018). \\n\\n------------\\n\\n 8 Eerola, T., and J. K. Vuoskoski. 2013. “A review of music and emotion studies: Approaches, emotion models, and stimuli.” Music Perception 30(3):307–340. Ellis, D. P. 2007. “Beat tracking by dynamic programming.” J. New Music Res., 36(1), pp. 51–60.  Eppler, A., A. Mäncchen, J. Abesser, C. Weiss, and K. Frieler. 2014. “Automatic style classification of jazz recordings with respect to rhythm, tempo, and tonality.” Proceedings of the 9th Conference on Interdisciplinary Musicology – CIM14. Berlin, Germany 2014. Eronen, A. J., and A. P. Klapuri. 2010. “Music Tempo Estimation With k-NN Regression.” IEEE Transactions on Speech and Audio Processing, 18 (1). Faraldo, A., Jorda, S., and Herrera, P. 2017. “A multi-profile method for key estimation in edm.” Proceedings of the AES International Conference on Semantic Audio. Erlangen, Germany: Audio Engineering Society.  Foote, J., M. L. Cooper, and U. Nam. 2002. “Audio retrieval by rhythmic similarity.” In Proc. of ISMIR (International Society for Music Information Retrieval), Paris, France, 2002. Foroughmand, H. and G. Peeters. (2019). “Deep-rhythm for tempo estimation and rhythm pattern recognition.” In Proc. of ISMIR (International Society for Music Information Retrieval), Delft, The Netherlands. Gainza, M., and E. Coyle. 2011. “Tempo detection using a hybrid multiband approach.” IEEE Transactions on Speech and Audio Processing, 19(1), 191-196. Gkiokas, A., V. Katsouros, and G. Carayannis. (2012). Reducing tempo octave errors by periodicity vector coding and svm learning. In Proc. of ISMIR (International Society for Music Information Retrieval), Porto, Portugal.  Gkiokas, A., V. Katsouros, G. Carayannis, and T. Stafylakis. 2012. “Music tempo estimation and beat tracking by applying source separation and metrical relations.” in Proc. Int. Conf. Acoust., Speech, Signal Process., pp. 421-424. Goodchild, M., B. Gingras, and S. McAdams. 2016. “Analysis, performance, and tension perception of an unmeasured prelude for harpsichord.” Music Perception 34(1):1–20. Goto, M. 2001. “An audio-based real-time beat tracking system for music with or without drum-sounds.” Journal of New Music Research,  pp. 159–171. Gouyon, F., A. P. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis, C. Uhle, and P. Cano. 2006. “An experimental comparison of audio tempo induction algorithms.“ IEEE Transactions on Audio, Speech, and Language Processing, 14(5):1832– 1844. 2006.  Grzywczak, D., and Gwardys, G. 2014. “Audio features in music information retrieval.” In D. Slezak, G. Schaefer, S. T. Vuong, and Y-S. Kim (Eds.) Proceedings of Active Media Technology, 10th International Conference, Warsaw, Poland, August 11–14, 2014. Hainsworth, S. W. 2004. “Techniques for the automated analysis of musical audio.” PhD thesis, University of Cambridge, UK, September 2004. Han, Y., J. Kim, and K. Lee. 2016. “Deep convolutional neural networks for predominant instrument recognition in polyphonic music.” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25(1), 208-221. Holzapfel, A., and Y. Stylianou. 2011. “Scale transform in rhythmic similarity of music.” IEEE Transactions on Audio, Speech and Language Processing, 19(1):176–185, 2011.  Holzapfel, A., M. E. P. Davies, J. R. Zapata, J. L. Oliveira, and F. Gouyon. 2012. “Selective sampling for beat tracking evaluation.” IEEE Transactions on Audio, Speech, and Language Processing, 20(9), 2539–2548, 2012.  Hörschläger, F., R. Vogl, S. Böck, and P. Knees. 2015. “Addressing tempo estimation octave errors in electronic music by incorporating style information extracted from Wikipedia”, In Proc. of the Sound and Music Computing Conference (SMC), Maynooth, Ireland, 2015. Istvanek, M. 2021. “The application of tempo calculation for musicological purposes.” Doctoral Degree Programme (2), FEEC BUT. DOI: 10.13164/eeict.2021.265 Klapuri, A., A. J. Eronen, and J. T. Astola. 2006. “Analysis of meter of acoustic music signals.” IEEE transactions on audio, speech, and language processing, 14(1), 342–355. Knees, P., Á. Faraldo, P. Herrera, R. Vogl, S. Böck, F. Hörschläger, and M. Le Goff. 2015. “Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections.” In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR), pp 364–370, Málaga, Spain, October 2015.  Kraft, S., A.Lerch, and U. Zölzer. 2013. “The tonalness spectrum: feature-based estimation of tonal components.” Proceedings of the 16th International Conference on Digital Audio Effects (DAFx), Maynooth, Ireland. Lartillot, O., and P. Toiviainen. 2007. “A Matlab toolbox for musical feature extraction from audio.” In Proceedings of the 10th International Conference on Digital Audio Effects, pp. 237-244. Levy, M. (2011). Improving perceptual tempo estimation with crowd-sourced annotations. In Proc. of ISMIR (International Society for Music Information Retrieval), Miami, Florida, USA.  Li, T. LH., and A. B. Chan. 2011. “Genre classification and the invariance of MFCC features to key and tempo.” In International Conference on MultiMedia Modeling pp. 317–327. Marchand, U., and G. Peeters. 2014. “The modulation scale spectrum and its application to rhythm-content description.” In Proc. of DAFx (International Conference on Digital Audio Effects), Erlangen, Germany, 2014.  Marchand, U., Fresnel, Q., and Peeters, G. (2015). “GTZAN-Rhythm: Extending the GTZAN Test-Set with Beat, Downbeat and Swing Annotations.” Late-Breaking Demo Session of the 16th International Society for Music Information Retrieval Conference, 2015.  Marchand, U., and G. Peeters. 2016. “Scale and shift invariant time/frequency representation using auditory statistics: Application to rhythm description.” In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2016.  Marchand, U., and G. Peeters. 2016 b. “The extended ballroom dataset.” Late Breaking Demo of the International Conference on Music Information Retrieval (ISMIR), New York, NY, USA. Müller M., V. Konz, A. Scharfstein, S. Ewert, and M. Clausen. 2009. “Towards automated extraction of tempo parameters from expressive music recordings.\" In: k. Hirata, G. Tzanetakis, K. Yoshii K (Eds.) ISMIR 2009, 10th international conference on music information retrieval. Oliveira, J. L., F. Gouyon, L. G. Martins, and L. P. Reis. 2010. “IBT: A real-time tempo and beat tracking system,” in Proc. Int. Soc. For Music Information Retrieval (ISMIR), pp. 291– 296, 2010. Oyama, T., R. Ishizuka, and K. Yoshii. 2021. “Phase-aware joint beat and downbeat estimation based on periodicity of metrical structure.” in Proc. of the 22nd Int. Society for Music Information Retrieval Conf., Online, 2021. Peeters. G. 2011. “Spectral and temporal periodicity representations of rhythm for the automatic classification of music audio signal.” IEEE Transactions on Audio, Speech and Language Processing, 19(5):1242– 1252, 2011.  Peeters, G., and J. Flocon-Cholet. 2012. “Perceptual tempo estimation using GMM-regression.” In Proceedings of the second international ACM workshop on Music information retrieval with user-centered and multimodal strategies (MIRUM), pp 45–50, New York, NY, USA, 2012. ACM.  Percival, G. and Tzanetakis, G. (2014). “Streamlined tempo estimation based on autocorrelation and cross-correlation with pulses.” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(12):1765–1776.  Phan, H., L. Hertel, M. Maass, and A. Mertins. 2016. “Robust audio event recognition with 1-max pooling convolutional neural networks.” arXiv preprint arXiv:1604.06338. Pons, J., T. Lidy, and X. Serra. 2016. “Experimenting with musically motivated convolutional neural networks.” In 14th International Workshop on Content- Based Multimedia Indexing (CBMI). IEEE. Pons, J., and X. Serra. 2017. “Designing efficient architectures for modelling temporal features with convolutional neural networks.” In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 2472-2476). IEEE. Raffel, C. (2016). Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching. PhD thesis, Columbia University.  Sachs, C. 1953. Rhythm and Tempo. London: Dent. Scheirer, E. D. 1998. “Tempo and beat analysis of acoustic musical signals.” J Acoust. Soc. Amer., 103(1), 588–601. \\u2028  Schlüter, J., and S. Böck. 2014. “Improved musical onset detection with convolutional neural networks.” In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. Schreiber. H. 2017. “CNN-based automatic musical key detection.” Music Information Retrieval Evaluation eXchange (MIREX), Suzhou, China. Schreiber, H. and Müller, M. (2017). A post-processing procedure for improving music tempo estimates using supervised learning. In Proc. of ISMIR (International Society for Music Information Retrieval), Suzhou, China.  \\n\\n------------\\n\\n 9 Schreiber, H., and M. Müller. 2018. “A single-step approach to musical tempo estimation using a convolutional neural network.” In 19th International Society for Music Information Retrieval Conference, Paris, France, 2018. Schreiber, H., and M. Müller. 2018 b. “A crowdsourced experiment for tempo estimation of electronic dance music.” Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR), Paris, France, September 2018. Schreiber, H., and M. Müller. 2019 a. “Musical tempo and key estimation using convolutional neural networks with directional filters.” arXiv preprint arXiv:1903.10839. Schreiber, H., J. Urbano, and M. Müller. 2019 b. Music tempo estimation: Are we done yet? Transactions of the International Society for Music Information Retrieval, 3(1), 111–125. Schreiber, H., F. Zalkow, and M. Müller. 2020. “Modeling and estimating local tempo: A case study on Chopin’s Mazurkas.” In Proc. of the 21st Int. Society for Music Information Retrieval Conf., Montréal, Canada, 2020.   Seyerlehner, K., Widmer, G., and Schnitzer, D. (2007). From rhythm patterns to perceived tempo. In Proc. of ISMIR (International Society for Music Information Retrieval), Vienna, Austria.  Tzanetakis, G., and P. Cook. 2002. “Musical genre classification of audio signals.” IEEE Transactions on Speech and Audio Processing, 10(5):293–302, 2002. van den Oord, A., S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. 2016. “Wavenet: A generative model for raw audio.” arXiv preprint arXiv:1609.03499. Webster, G. D. and C. G. Weir. 2005. “Emotional responses to music: Interactive effects of mode, texture, and tempo.” Motivation and Emotion 29(1):19-39. Wu, F-H. F. 2015. “Music tempo octave error reducing based on the statistics of tempogram.” 23rd Mediterranean Conference on Control and Automation (MED) June 16-19, 2015. Torremolinos, Spain. Wu, F-H. F. and J-S. R. Jang. 2014. “A supervised learning method for tempo estimation of musical audio”, In 22nd Mediterranean Conference of Control and Automation (MED), 2014, 599–604. IEEE, 2014. Wu, F.-H. F., T-C. Lee, J-S. R. Jang, K. K. Chang, C.-H. Lu, and W-N. Wang, 2011. “A two-fold dynamic programming approach to beat tracking for audio music with time-varying tempo.” in International Society for Music Information Retrieval Conference (ISMIR), 2011, pp. 191–196. Xiao, L., Tian, A., Li, W., and Zhou, J. (2008). “Using statistic model to capture the association between timbre and perceived tempo.” In Proc. of ISMIR (International Society for Music Information Retrieval), Philadelphia, PA, USA. Zapata, J. R., and Gómez, E. 2011. “Comparative evaluation and combination of audio tempo estimation approaches.” In Audio engineering society conference: 42nd international conference: Semantic audio. Audio Engineering Society.  i https://pypi.org/project/madmom/ '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = chained_store['AI and Tempo Estimation.pdf']\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using postget to change decoding according to extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save_here.json',\n",
       " 'nested.json',\n",
       " 'AI and Tempo Estimation.pdf',\n",
       " 'JAMMIN-GPT.pdf',\n",
       " 'simple.docx',\n",
       " 'Release Notes.docx',\n",
       " 'simple.txt']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dol import Files\n",
    "from dol_cookbook import misc_files_path\n",
    "\n",
    "s = Files(misc_files_path)\n",
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save_here.json',\n",
       " 'nested.json',\n",
       " 'AI and Tempo Estimation.pdf',\n",
       " 'JAMMIN-GPT.pdf',\n",
       " 'simple.docx',\n",
       " 'Release Notes.docx',\n",
       " 'simple.txt']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfdol.base import bytes_to_pdf_reader_obj, read_pdf_text\n",
    "from msword import bytes_to_doc, get_text_from_docx\n",
    "\n",
    "extension_to_decoder = {\n",
    "    '.txt': lambda obj: obj.decode('utf-8'),\n",
    "    '.json': json.loads,\n",
    "    '.pdf': Pipe(\n",
    "        bytes_to_pdf_reader_obj, read_pdf_text, '\\n\\n------------\\n\\n'.join\n",
    "    ),\n",
    "    '.docx': Pipe(bytes_to_doc, get_text_from_docx),\n",
    "}\n",
    "\n",
    "from dol import wrap_kvs, Pipe\n",
    "\n",
    "def extension_based_decoding(k, v):\n",
    "    ext = '.' + k.split('.')[-1]\n",
    "    decoder = extension_to_decoder.get(ext, None)\n",
    "    if decoder is None:\n",
    "        raise ValueError(f\"Unknown extension: {ext}\")\n",
    "    return decoder(v)\n",
    "\n",
    "def extension_base_wrap(store):\n",
    "    return wrap_kvs(store, postget=extension_based_decoding)\n",
    "\n",
    "store = extension_base_wrap(s)\n",
    "list(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Just a bit of text to show that is works. Another sentence.\\nThis is after a newline.\\n\\nThis is after two newlines.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store['simple.docx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': 37,\n",
       " 'examples': {'apple': ['pie', 'crumble', 'sauce'], 'banana': 'split'}}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store['nested.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1 AI and Tempo Estimation: A Review Geoff Luck1 1 Centre of Excellence in Music, Mind, Body and Brain, Department of Music, Art and Culture Studies, University of Jyväskylä, Finland.  1geoff.luck@jyu.fi   Abstract The author’s goal in this paper is to explore how artificial intelligence (AI) has been utilized to inform our understanding of and ability to estimate at scale a critical aspect of musical creativity — musical tempo. The central importance of tempo to musical creativity can be seen in how it is used to express specific emotions (Eerola and Vuoskoski 2013), suggest particular musical styles (Li and Chan 2011), influence perception of expression (Webster and Weir 2005) and mediate the urge to move one’s body in time to the music (Burger et al. 2014). Traditional tempo estimation methods typically detect signal periodicities that reflect the underlying rhythmic structure of the music, often using some form of autocorrelation of the amplitude envelope (Lartillot and Toiviainen 2007). Recently, AI-based methods utilizing convolutional or recurrent neural networks (CNNs, RNNs) on spectral representations of the audio signal have enjoyed significant improvements in accuracy (Aarabi and Peeters 2022). Common AI-based techniques include those based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)), classification and statistical learning (e.g., support vector machines (SVM)), and artificial neural networks (ANNs) (e.g., self-organizing maps (SOMs), CNNs, RNNs, deep learning (DL)). The aim here is to provide an overview of some of the more common AI-based tempo estimation algorithms and to shine a light on notable benefits and potential drawbacks of each. Limitations of AI in this field in general are also considered, as is the capacity for such methods to account for idiosyncrasies inherent in tempo perception, i.e., how well AI-based approaches are able to ‘think and act like humans.’ Introduction Artificial Intelligence (AI) can be defined as the simulation of human intelligence in computer systems programmed to think and act like humans (Russell, Norvig and Crouch 2010). The rapid development of AI and related technologies has had profound effects on a range of everyday and high-level activities and industries. From finance to healthcare, shopping, and transport, AI and its constituent and associated methods have opened a Pandora’s box of opportunity across a growing array of societal structures, including commerce, government, and academia. In particular, the impact of AI on creative processes marks perhaps the most significant incursion yet into what we conceptualize as true human activity.  Music is one such creative activity, a universal found in some form in all known human cultures. And recent work has revealed the potential for AI to transform how we understand, evaluate, and model creative musical processes. From composition to performance, recommendation to transcription, AI has been shown to have a range of impacts on this most human of creative endeavors. The author’s goal in this paper is to explore how AI has been utilized to inform our understanding of, and our ability to estimate at scale, one particular, fundamental aspect of musical creativity — musical tempo. Musical Tempo Tempo, including its absence, is a fundamental characteristic of any piece of music. Typically indicated in beats per minute (bpm), tempo refers to the speed or pace a musical work is (or is intended to be) played at. The fundamental importance of musical tempo can be seen in how it is utilized by composers and performers to express specific emotions (Eerola and Vuoskoski 2013), to suggest particular musical styles (Li and Chan 2011), and to build and release tension (Goodchild, Gingras and McAdams 2016). From a listeners’ perspective, musical tempo can influence perception of expression (Webster and Weir 2005), level of arousal (Lundqvist, Carlsson, Hilmersson, and Juslin 2009), as well as the urge to move one’s body in time to the music (Burger et al. 2014).  Musical tempo can be understood as representing two distinct concepts: A physical concept referring to the number of events produced per minute or a psychological concept corresponding to the number of events perceived per minute. The latter can be conceptualized as the rate at which a typical listener would tap or move along to a piece of music (Drake, Gros, and Penel 1999; Sachs 1953). The majority of scientific work focuses on perceptual tempo. Indeed, in the field of music information retrieval (MIR), tempo appears to be implicitly understood as the rate at which a listener would tap along to the music (Fraisse 1982). While there exists considerable individual variation in how listeners define the exact tempo of a piece of music (e.g., at which beat level or octave they tap along to), reliable estimation of perceived tempo across large corpora of music remains a key goal in MIR (Böck 2010). This is because tempo plays a crucial role in a range of applied MIR-related pursuits, including music classification (Nieto 2020), genre recognition (Tzanetakis and Cook 2002), emotion analysis (Cambouropoulos 2000), algorithmic generation (Mauch, Durieux, Müller and Riedl 2015), transcription (Boeck and Widmer 2014), sound source separation (Uhlich, Kim and Lee 2017), and recommendation (Zhang et al. 2018). The significance of tempo in the MIR community is further emphasized in the recurring competitions held between tempo extraction algorithms over the past two decades (Downie 2008).  Performance of an algorithm is typically assessed according to standardized metrics such as Accuracy0 (Acc0), Accuracy1 (Acc1), Accuracy2 (Acc2), and, where relevant, ClassAccuracy. Acc0 evaluates the number of (rounded) estimated tempo values that are identical to an annotated ground truth; Acc1 evaluates if estimated tempo lies within \\n\\n------------\\n\\n 2 +/- 4% of the annotated ground truth; Acc2 is the same as Acc1 but considers octave errors – confusing the actual tempo to its rhythmic counterparts – as correct; ClassAccuracy evaluates ability to correctly classify tempo from a range of classes where a classification approach is implemented. In terms of frameworks utilized, tempo estimation methods can be roughly divided into traditional approaches and more recent, AI-based methods. Traditional tempo estimation methods typically detect signal periodicities that reflect the underlying rhythmic structure of the music (Grzywczak and Gwardys 2014), often using some form of autocorrelation of the amplitude envelope (Toiviainen and Lartillot 2007). Examples of autocorrelation-based tempo estimation algorithms include the Miningsuite (Lartillot 2019) and marsyas (Tzanetakis and Percival 2013). The former enjoys widespread use in areas such as music psychology, the latter in the field of MIR. Both have achieved good results in tempo detection tasks and have been utilized in distinct areas of music information research, including both industry and academia. An alternative to these algorithms is to crawl tempo metadata via Spotify\\'s API. Although Spotify uses a proprietary algorithm not available to the public, it has nonetheless grown to be an industry standard with many uses also in academia. An open-source alternative is librosa (McFee et al. 2015), a Python package that can be used to extract temporal and spectral features from audio. Accuracy of these different possibilities varies, and none should be considered the de facto standard. More recently, AI-based methods utilizing convolutional neural networks (CNNs) or recurrent neural networks (RNNs) on spectral representations of the audio signal have enjoyed significant improvements in accuracy over their traditional counterparts (Aarabi and Peeters 2022). Examples of models employing convolutional and recurrent neural networks include Schr (Schreiber and Müller 2018) and böck (Böck, Krebs and Widmer 2015), respectively. Other AI-based techniques include those based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)), classification and statistical learning (e.g., support vector machines (SVMs)), and artificial neural networks (ANNs) (e.g., self-organizing maps (SOMs), CNNs, RNNs, DL). This is neither an exhaustive list nor the sole framework within which to categorize these approaches. It does, however incorporate many of the most common methods. Related to tempo estimation is recognition of rhythm patterns. The latter is really an extension of the former but has received less attention in the literature compared to tempo recognition. This is likely due to the difficulty in creating datasets that are annotated for rhythm patterns, since distinguishing between similar patterns remains a difficult task. As with tempo estimation, early systems for recognizing rhythm patterns used hand-crafted signal processing and statistical models, while more recent systems have used ML and DL techniques. Examples of these include systems that utilise a beat spectrum (Foote 2002), beat histogram (Tzanetakis 2002), harmonic analysis (Peeters 2011), and scale transform (Holzapfel 2011). The latter approach was extended by Marchand and Peeters (2014, 2016) by combining it with the modulation spectrum and adding correlation coefficients between frequency bands. What follows is an overview of some principal applications of these approaches to modeling and estimating perceived musical tempo and related tasks such as rhythm recognition and beat tracking. The object is to describe in general terms – and examine the level of accuracy achieved by – AI-based tempo estimation algorithms, as well as to shine a light on notable benefits and potential drawbacks of different approaches. Limitations of the use of AI in this field in general are also considered. Attention is paid to how well AI-based methods account for idiosyncrasies inherent in tempo perception, i.e., how well they are able to ‘think and act like humans.’ Gaps in the literature and future directions for research are also highlighted. AI and Musical Tempo 1. Reducing Octave Errors Traditional tempo estimation typically starts with detecting rhythmically related events and then estimating tempo by finding the dominant periodicity of the onsets related to the beat positions (Hainsworth 2004). Various techniques have been used for the latter aspect, including autocorrelation (Percival and Tzanetakis 2014), comb filters (Scheirer 1998), dynamic programming (Ellis 2007), Hidden Markov Models (Klapuri, Eronen and Astola 2006), and source separation (Gkiokas, Katsouros, Carayannis and Stafylakis 2012). Despite sizeable performance increases over the years, most of these approaches still suffer from octave errors. Many attempts have been made to reduce such errors by using techniques such as Gaussian Mixture Models (GMM) (Peeters and Flocon-Cholet 2012), neural networks (Böck, Krebs and Widmer 2015), k-nearest neighbour classification (k-NN) (Wu and Jang 2014), genre classification (Hörschläger, Vogl, Böck and Knees 2015), and Support Vector machines (SVM) (Percival and Tzanetakis 2014). Such methods have often been applied as a separate stage in a multi-stage approach. For instance, some authors have introduced a machine learning (ML)-driven classification step to help guide the algorithm to the correct tempo octave. Chen, Cremer, Lee, DiMaria, and Wu (2009) took a novel approach to this by using musical descriptors related to perceived mood to train a statistical model of perceived tempo classes in a unique 299-track dataset. Using an SVM and ground truth ratings as input, each track was pre-classified as being either 1) very slow, 2) somewhat slow, 3) somewhat fast, 4) very fast. The logic was that this first-stage classification could then be used to improve the accuracy of any conventional tempo estimation algorithm. To illustrate this, they tested the effect of their first-stage octave classifier in combination with a range of existing algorithms on the ISMIR04 and MIREX06 datasets. The algorithms selected were the 11 submitted to the ISMIR04 tempo estimation competition plus a new algorithm developed by the authors. Results revealed that the authors’ octave-corrected approach indeed led to a significant reduction in octave errors with these algorithms. In fact, performance across all algorithms improved by an average of 45% and 65.5% for the ISMIR04 and MIREX06 datasets, respectively. It’s worth emphasizing again that there was no additional low-level analysis of temporal events or repetition rates in the audio signal of each track: The corrective procedure was thus entirely independent of tempo-specific \\n\\n------------\\n\\n 3 information. This represents a major benefit of Chen et al.’s (2009) system. In addition, the fact that it can be used to improve any traditional tempo estimation algorithm makes it a very versatile approach. Nonetheless, even with the correction, only three of the algorithms tested achieved Acc1 performance above 50%. Wu (2015) also tackled the octave error problem by estimating tempo using a two-stage process, albeit in a slightly different fashion. First, the two most dominant tempi were estimated from a tempogram obtained from the short time Fourier transform of an onset detection function (ODF). Second, a k-NN or SVM classifier was used to discriminate the predominant tempo from the pair so identified. Wu (2015) evaluated this approach with the ISMIR04 Ballroom and Songs datasets. Results revealed a reduced rate of octave errors compared to previous approaches. Specifically, Wu achieved high levels of performance on Acc1 measures for both Ballroom (78.5%) and Songs (62.6%). In fact, both of these accuracy rates were superior to 7 tested alternatives – Klapuri (Gouyon et al 2006; Eronen and Klapuri 2010), Gkiokas-MA (Gkiokas, Katsourus and Carayiannis 2012), Gkiokas (Gkiokas et al 2012), Uhle (Gouyon et al 2006), Scheirer (Gouyon et al 2006; Scheirer 1998), Gainza-Hyb1 (Gainza and Coyle 2011), Gainza-Hyb2 (Gainza and Coyle 2011). For Acc2, performance was again superior to the same 7 alternatives for Ballroom (95%), but not for Songs (80.6%). The main benefit of Wu’s approach was generally superior performance compared to a range of previous approaches on the two databases selected for testing. The principal drawbacks were that 1) only a small number of datasets were tested, and 2) because of the relative homogeneity of the datasets utilized, it’s unclear how the system would perform on more genre-diverse collections. In a similar vein, Dutta (2018) framed octave error correction as a classification problem, and proposed a four-stage ‘tempo estimation plus octave correction’ architecture termed base+octv. Stage 1 involved detecting onsets; Stage 2 entailed beat period detection; Stage 3 involved Histogram building; and Stage 4 implemented octave classification via an SVM. The SVM utilized 5 features and a non-linear kernel to classify tempo into one of 3 octave classes. Like similar approaches, the goal was to reduce octave errors. Acc1 performance on 6 classic datasets (Ballroom, Hainsworth, ACM_MIRUM, GTZAN, ISMIR04, SMC_MIRUM) was found to be superior to a range of existing approaches (Percival (Percival and Tzanetakis 2014), Klapuri (Klapuri, Eronen and Astola 2006), Gkiokas (Gkiokas et al. 2012), IBT (Oliveiraet al. 2010), qm_vamp (Davies and Plumbley 2007), Scheirer (Scheirer 1998)), while Acc2 was found to be on-par with though not better than the state-of-the-art. Specifically, Acc1 performance was 69.6% (slightly better than the state of the art), while Acc2 performance was 91.2% (on-par with but not better than the state of the art). Dutta argues that their approach might classify unseen audio files better. However, they provide no evidence to support this assertion. The principal drawback is thus that this system was not tested on unseen data. The approaches outlined above, then, suggest that one way of reducing octave errors is to use classification techniques prior to or after principal tempo estimation to limit the range of possible tempos. The most versatile implementation of this approach seems to be that described by Chen et al (2009) since their classifier can apparently function in combination with any tempo estimation algorithm. Despite classification-based octave error reduction techniques improving performance of (mostly) traditional tempo estimation techniques, the field evolved with the development of neural network-based approaches and the framing of tempo estimation itself as a classification problem. 2. Tempo Estimation Itself as a Classification Problem More recently, the traditional-plus-octave-error-reduction approach has been largely supplanted by steadily-improving architectures based on neural networks. Here, the estimation of tempo itself is framed as a classification problem. Authors have made significant strides utilizing CNNs for this purpose. This work was born out of the use of CNNs in non-MIR fields, such as image classification. The widespread analysis of spectrograms in MIR research, and the visual nature of such time-frequency representations of music, led to standard computer vision CNNs being implemented to ‘see’ and classify events in spectrograms (e.g., Choi, Fazekas and Sandler 2016; Phan, Hertel, Maass and Mertins 2016; Han, Kim and Lee 2016). It seems noteworthy that most MIR DL scientists used spectrograms as input to their CNNs (Choi et al. 2016; Phan et al. 2016; Han et al. 2016; Pons, Lidy and Serra 2016; Schlüter and Böck 2014). Most even adopted stock rectangular filters for classification. This latter point highlights potential shortcomings of this approach: While images have spatial meaning, spectrograms’ dimensions represent time and frequency. Consequently, wider or higher filters might be capable of learning longer temporal dependencies in the audio domain or timbral features spread across a wider range of frequencies, respectively. The result would be musically- as opposed to visually-motivated filter shapes. Indeed, this is precisely what encouraged Pons and Serra (2017) to investigate whether MIR CNNs might benefit from a design oriented towards learning musical features rather than ‘seeing’ spectrograms, i.e., filter shapes adapted to musical concepts.  Pons and Serra (2017) proposed a novel design strategy for convolutional neural networks (CNNs) in music classification tasks, specifically spectrogram analysis. Their approach was to use different filter shapes adapted to fit musical concepts within the first layer. This is more expressive and certainly intuitive than the default small rectangular filters typically used. Pons & Serra (2017) developed a shallow approach in which two complimentary architectures designed to model onsets (O-net) or patterns (i.e., rhythm, tempo) (P-net) using shorter and longer filters, respectively, were implemented in parallel. By systematically manipulating combinations of the two with different parameter settings, the authors derived 8 different approaches. Performance of these approaches on the Ballroom dataset was then compared against 3 DL-based approaches – Time (Pons et al. 2016), Time-freq (Pons et al. 2016), Black-box (Pons et al. 2016) – and 1 non-DL approach – Marchand et al. (Marchand and Peeters, 2016). Results showed that Pons & Serra’s (2017) strategy was useful for fully exploiting the representational capacity of the first CNN layer when modelling music. Specifically, their best approach scored \\n\\n------------\\n\\n 4 second only to Marchand et al. in terms of tempo estimation accuracy. Building on Pons and Serra’s (2017) work, Schreiber and Müller (2019) further developed the use of CNNs for musical tempo estimation by exploiting the different semantics of spectrograms\\' time and frequency axes. Train/test datasets utilized were Ballroom (Schreiber and Müller 2018), EBall (Schreiber and Müller 2018; Gouyon et al. 2006; Marchand and Peeters 2016 b), GiantSteps Key (Knees et al. 2015), GiantSteps Tempo (Knees et al. 2015; Schreiber and Müller 2018 b), GTzan Key (Tzanetakis and Cook 2002; Kraft, Lerch and Zölzer 2013), GTzan Tempo (Tzanetakis and Cool 2002; Percival and Tzanetakis 2014), LMD Key (Raffel 2016; Schreiber 2017), LMD Tempo (Schreiber and Müller 2018; Raffel 2016), and MTG Tempo/MTG Key (Schreiber and Müller 2018; Faraldo et al. 2017). Employing a range of approaches, from shallow, domain-specific to deep variants with directional filters, they found that axis-aligned architectures performed on par with VGG-style networks developed for computer vision. At the same time, they were both less affected by confounding factors and required fewer model parameters. The adoption of neural networks by the MIR community, then, significantly increased the capabilities of tempo estimation algorithms. In particular, approaches that moved beyond ‘seeing’ spectrograms to applying directional filters based on musical concepts upped the overall level of performance in the field while at the same time simplifying the architectures. Still, these approaches largely only focused on estimating one musical characteristic – tempo – at a time. What came next were approaches able to estimate multiple characteristics – such as tempo, beat, and downbeat – simultaneously. 3. Multi-Task Methods for Simultaneous Estimation of Multiple Characteristics As we’ve seen so far, early tempo estimation methods based on the application of signal processing techniques such as autocorrelation analysis, comb filtering, and the discrete Fourier transform (DFT), to onset strength signals (OSS) extracted from the audio (e.g., Percival and Tzanetakis 2014; Böck, Krebs and Widmer 2015; Wu, Lee, Jan, Chang, Lu and Wang 2011) suffered from frequent octave confusion. More recently, deep neural networks (DNNs) used for direct tempo estimation, in which the task was framed as a classification problem (Schreiber and Müller 2018; Foroughmand and Peeters 2019) increased overall level of performance. More recently still, multi-task methods have been developed for joint estimation of multiple metrical elements, such as beat and downbeat (Goto 2001; Böck, Krebs and Widmer 2016) and beat, downbeat, and tempo (Böck, Davies and Knees 2019; Böck and Davies 2020). The overlap of the problems within these multi-task approaches have led them to achieve high levels of performance using innovations such as Long Short-Term Memory (LSTM) and Temporal Convolutional Networks (TCNs), as well as exploiting the benefits of training with data augmentation.  TCNs, which first appeared in the WaveNet generative audio model (van den Oord et al. 2016), were proposed by Davies and Böck (2019) as an alternative to a CNN approach using Bidirectional Long Short-Term Memory (BLSTM) for audio-based beat tracking. The authors observed that TCNs achieved state-of-the-art performance on a wide range of existing beat tracking datasets. They were also well suited to parallelization, allowing them to be trained efficiently even on very large datasets. Moreover, they required only a small number of weights. According to the authors, these attributes made TCNs a promising choice for audio-based beat tracking tasks. Böck, Davies and Knees (2019) built upon Davies and Böck’s (2019) beat-tracking system underpinned by temporal convolutional networks (TCNs). The authors proposed a multi-task learning system that simultaneously tracked beats and estimated tempo. As in Davies and Böck (2019) the system used TCNs, but here globally aggregated the skip connections, feeding them into a tempo classification layer. The multi-task nature of the system allowed it to exploit the mutual information of both tasks (by definition, the two tasks are highly interconnected) and improve one by learning only from the other. To assess the approach, a range of existing annotated datasets were used for training (Ballroom, Beatles, Hainsworth, Simac, SMC, HJDB) and testing (ACM Mirum, GiantSteps, GTZAN), and performance was evaluated against four reference systems (Gkiokas et al., Percival and Tzanetakis, Böck et al., Schreiber and Müller). Tempo estimation was evaluated with Acc1 and Acc2 measures. For both evaluation criteria, the multi-task approach achieved state-of-the-art performance at (both beat tracking and) tempo estimation. In particular, the system demonstrated improved performance on beat-tracking when trained with data that included tempo-only annotations. In other words, much like humans, the system learnt from information provided concerning a different but related task. However, no mention was made of tempo estimation performance when only beat-tracking data was included. Böck et al (2019) suggested that this approach may have a significant impact on beat tracking moving forward, as it allows for the use of alternative training data (global tempi) that are more prevalent and easier to annotate. The authors also discussed the computational benefits of the proposed approach, which include efficient training and reduced over-fitting. One potential drawback of Böck et al’s approach is that no mention is made of tempo estimation performance when only beat-tracking data is included. This isn’t strictly a negative aspect of the system, it’s just that no information is provided to clarify this. In terms of being human-like, the authors suggest that further research should be conducted on this \"compact\" deep model approach for generalization capabilities and re-use of information for end-users on unseen data. In a conceptual development of Davies and Böck (2019), Oyama, Ishizuka and Yoshii (2021) proposed a phase-aware method for jointly estimating beat and downbeat in popular music. They utilised a deep neural network (DNN) that estimated the beat phase at each frame instead of the beat presence probability. Their approach used all frames for training, not just a limited number of beat frames. The authors also modified the post-processing method for the estimated phase sequence. Different multi-task learning architectures for joint beat and downbeat detection were investigated, and the experimental results demonstrated the importance of phase modelling for stable beat and downbeat estimation. \\n\\n------------\\n\\n 5 A further multi-task development was proposed by Böck and Davies (2020).  They described a state-of-the-art DNN that simultaneously estimated tempo, beat location, and downbeat location. In particular, they used a data augmentation approach to expose their network to a wider range of information pertaining to these three aspects. Data augmentation, of course, is a major benefit of AI-oriented approaches in general, allowing a larger and more diverse dataset to be effectively created from a smaller more homogeneous one. The result of Böck and Davies’ approach was a performance increase of up to 6% over existing approaches.  The data augmentation argument above notwithstanding, large datasets of tempo-relevant annotations have facilitated development of so-called ‘data-driven’ tempo estimation in which ML algorithms learn from the annotated data. Initial developments in the field utilized algorithms based on approaches including bags of classifiers (Levy 2011), Gaussian Mixture Models (GMM) (Xiao et al. 2008; Peeters and Flocon-Cholet 2012), k-Nearest-Neighbors (k-NN) (Seyerlehner et al. 2007), Random Forests (Schreiber and Müller 2017), and SVMs (Chen et al. 2009; Gkiokas et al. 2012; Percival and Tzanetakis 2014). More recently, DL approaches have come to dominate the ML space.  An early DL-based tempo estimation system was that proposed by Böck, Krebs and Widmer (2015). This applied a bank of resonating comb filters to the output of an RNN to predict beat position then predict tempo from the estimated periodicity of the signal. Their approach did not use hand-crafted features. Instead, the authors used an intermediate beat-level representation of the signal as input to the comb filter bank. No complex post-processing was applied. Instead, the output was simply the highest resonator\\'s histogram peak. Böck et al’s approach achieved state-of-the-art performance on nine out of the ten datasets tested. An alternate method was proposed by Schreiber and Müller (2018) that swapped the comb filter for a mel-spectrogram-based approach again applied to a CNN. The latter approach framed tempo prediction as a classification task into tempo classes.  More recently, Foroughmand and Peeters (2019) developed a hybrid approach combining tempo- and genre-related information into a ‘hand-crafted-plus-data-driven’ approach they called Deep Rhythm (DR). DR represented a significant development because it simultaneously estimated tempo and classified rhythm patterns/genres. To do this, the authors proposed a new representation of the DNN input called harmonic constant-Q modulation (HCQM) that accounted for tempo frequencies in the harmonic series. DR considers how tempo and rhythm pattern interact to more accurately model the audio input. HCQM represented the harmonic series of tempo candidates in audio signals using a 4D-tensor, which was then used as input for a convolutional network to perform tempo and rhythm pattern estimation. In testing across multiple datasets, Foroughmand and Peeters observed incremental improvements in Acc1 for tempo estimation. At the same time, DR outperformed previous approaches on Acc1 and Acc2 measures, though only for Ballroom (ballroom music) and Giant-steps tempo (electronic music) test sets. This suggests that, in line with its name, DR performs best when rhythm is more clearly defined. Thus, DR offered incremental improvements in Acc1 for tempo estimation. Multi-task methods, in which multiple characteristics are estimated simultaneously really demonstrate the power of AI-based approaches in estimating temporal features of music. This is especially true in cases, such as the approach described by Böck et al (2019), in which training on one data type, e.g., tempo annotations, improves performance on a related but different task, e.g., beat tracking. One aspect of tempo estimation not discussed so far, and that has also benefited extensively from AI-driven approaches, is estimation of local vs global tempo. 4. Estimation of Local vs. Global Tempo Most of the literature on tempo estimation, and all that reviewed so far, focuses principally on global tempo, i.e., the average tempo of an entire piece of music. This is likely because a steady, largely isochronous beat is a characteristic of a significant proportion of recorded music. The result is that, in most musical styles, especially of the popular era, tempo remains relatively unchanged throughout a track. This is particularly true of electronic styles such as EDM in which the beat is entirely machine-driven. However, not all music exhibits such tempo(ral) isochrony. More temporally expressive styles, notably those in classical-related genres, can exhibit huge deviations from an ‘average’ or global tempo. For these styles, estimation of local tempo – the tempo at different moments in time or covering shorter epochs – is critical. Several authors have focused either on estimating local tempo only or joint estimation of both local and global tempo.   Schreiber, Zalkow and Müller (2020), for instance, modelled local tempo in a selection of classical music pieces. As noted above, tempo is known to fluctuate significantly in such music. They found that CNN-based approaches quite accurately captured local tempo even for such expressive classical styles as long as they were trained on the target genre. Importantly, they observed that their results were very dependent on the specific training-test split selected. In a more sophisticated approach, Schreiber & Müller (2018) trained a CNN to estimate both local and global tempo in what they termed a single-step approach. In a traditional setup, note onsets or beats are first identified, from which tempo is then estimated. Schreiber & Müller’s approach instead framed tempo estimation as a multi-class classification problem. This permitted the use of a single-step method. Their CNN, trained on a large dataset covering a wide range of genres and tempi, was able to estimate tempo based on less than 12 seconds of audio as input. The ability to estimate tempo on such a relatively short sample of music made their algorithm particularly suitable (with caveats) to estimate not just global but also local tempo.  Schreiber and Müller (2018) compared their approach to the böck (Böck, Krebs and Widmer 2015) and schr (Schreiber and Müller 2018) algorithms using a standard range of datasets: ACM Mirum (Peeters and Flocon-Cholet 2012), Ballroom (Gouyon et al 2006), GTzan (Tzanetakis and Cook 2002), Hainsworth (Hainsworth 2004), ISMIR04 (Gouyon et al 2006), GiantSteps Tempo (Knees et al. 2015), and SMC (Holzapfel et al. 2012), the union of which they termed Combined. Their new approach achieved the highest results in \\n\\n------------\\n\\n 6 terms of the strict metrics Acc0 (44.8%) and Acc1 (74.2%). For octave-error tolerance (Acc2), new (92.1%) was slightly outperformed by both böck (93.6%) and schr (92.2%), but all approaches performed very well. Results suggested that new was better than böck at correctly estimating tempo octave, while böck and schr were better if the metrical level was ignored. In terms of Acc1, new was significantly better than both böck and schr for the Ballroom (92.0%), GiantSteps (73.0%), and ACM Mirum (79.5%) datasets. The finding that böck and schr outperformed new on the more genre-diverse GTzan and Hainsworth datasets suggested that genre-wide training would improve the latter’s results on other datasets as well. So, Schreiber and Müller’s single-step approach compared very favorably to other state-of-the-art techniques in terms of global tempo estimation. However, because it also performed well at local tempo estimation, it was found to be useful for identifying and visualizing tempo drift in musical performances. This latter aspect is particularly useful for music analysis purposes. Further benefits included the fact that it did not rely on handcrafted features, instead being completely data-driven. Perhaps the most notable advantage of the system, though, was how well it dealt with tempo octave confusion. The significant reduction in such errors perhaps demonstrates how well it ‘thought and acted like a human.’ There were, nonetheless, a few drawbacks to the system. First, since Jazz, Classical, and Reggae genres were missing from the training data, it remains unclear how it would perform on more genre-diverse datasets. Second, the authors note that the network architecture could be improved by reducing the number of parameters, such as the use of shorter filters, dilated convolutions, residual connections, and a suitable replacement for the fully connected layers. The benefit of these changes, of course, would be to reduce the number of operations needed for training and estimation, making the approach more efficient in the process. In another multi-method approach, Istvanek (2021) compared conventional and state-of-the-art methods of beat tracking. While most tempo estimation work focuses on music with a broadly isochronous pulse, this study tackled the more complex task of tracking string quartet music. The conventional system tested for this purpose was the beat tracking module in the Python librosa library (McFee et al. 2015). Like most earlier approaches, this system tracks periodicity in the onset strength envelope via modification of spectral difference or spectral flux. Such frameworks are known to perform poorly with rapidly changing tempi typical of Western art music, although this problem can be mitigated to some extent using an adaptive window size based not on a fixed number of input values but a fixed number of inter-onset-intervals (Müller, Konz, Scharfstein, Ewert, and Clausen 2009).  The state-of-the-art method tested was the madmom Python module.i This uses a bidirectional RNN with Long LSTM cells, the latter allowing the network to store information relating to longer-term temporal structure. The idea was that this should permit more accurate detection of beat location. Probability estimation of beat location within frames was accomplished via a dynamic Bayesian network (DBN) approximated by an HMM.  In comparing the two approaches, Istvanek (2021) found that while librosa was best at estimating average global tempo, the RNN-based madmom module offered a better representation of the rhythmic structure and detected the highest number of individual beats. One question Istvanek (2021) raises is whether neural network-based systems should not be considered conventional today. Given their widespread, even dominant use, this seems like a valid question to ask. Also, musicological analysis requires minimum time spent on manual editing and ground truth annotation. Thus, accurate representation of rhythmic structure, especially number and position of beats, is crucial in any beat tracking system for this type of analysis. With increasing complexity and power, AI approaches have brought us a long way in tempo estimation in little more than a decade. In particular, methods of simultaneous estimation of both multiple characteristics and tempo across multiple time frames (local, global) have proved extremely successful. Nonetheless, architectures continue to increase in complexity, and the current state-of-the-art can provide tempo estimations bordering on perfect. In the final section, three particular state-of-the-art approaches are surveyed to give a flavor of where AI-driven tempo estimation stands today. 5. Increasing Complexity and State-of-the-Art de Souza, Moura and Briot (2021) compared tempo estimation performance of two systems based on artificial neural networks — an architecture utilizing a Bidirectional Recurrent Neural Network (B-RNN) that takes as its input the Mel spectrogram and which outputs the estimated tempo class, and Schreiber and Müller’s (2018) Convolutional Neural Network (CNN) architecture. Both networks were trained on the same extensive database (12,550 tracks), including percussion-only tracks, and compared both with each other and with Schreiber and Müller’s original(ly trained) algorithm. Results revealed that the B-RNN model performed on-par with (though in most cases did not outperform) the CNN-based approaches, but was particularly accurate for percussion-only tracks. This suggests that rhythmic elements play a mediating factor in the ability of neural networks to learn and predict musical tempo. As the authors note, further research on this is hampered by a lack of percussion-focused databases for testing and training. Song and Wang (2022) paired a hidden-Markov model (HMM) with a periodic recurrent neural network (PRNN) in an attempt to reduce computational complexity in a beat-tracking task. A significant such reduction was achieved by exploiting the frequency contents of the music signal via Fourier transform. Compared to previous implementations of artificial networks such as bidirectional recurrent neural networks (Bi-RNN) and temporal neural networks (TCN), neither of which can perceive the frequency of the musical beat, the HMM-with-PRNN approach achieved close to state-of-the-art performance but with significantly lower computational cost. Indeed, in additional to the high level of performance, this lower computational cost with state-of-the-art performance is clearly the main benefit of Song and Wang’s (2022) approach. As discussed above, Foroughmand and Peeters (2019) developed a hybrid approach combining tempo- and rhythm pattern-related information into a ‘hand-crafted-plus-data-driven’ approach they called Deep Rhythm (DR). Recently, Aarabi and Peeters (2022) extended Deep Rhythm to \\n\\n------------\\n\\n 7 simultaneously estimate both tempo and genre of a musical signal. This is possible because tempo and genre are highly correlated aspects of music. The system used a harmonic representation of rhythm as input to a CNN, processed through complex-valued convolutions to consider relationships between frequency bands. A multitask learning approach was then used to jointly estimate tempo and genre. Additionally, a second input branch was added to the system, using a mel-spectrogram input dedicated to timbre, which improved the performance of both tempo and genre estimation.  The authors’ network was trained on 8596 tracks from 3 datasets (Extended Ballroom (EBR) (Marchand and Peeters 2016), tempo MTG (tMTG) (Faraldo et al. 2017), tempo LMD (tLMD) (Raffel 2016)) and tested on a total of 3611 tracks across 7 independent datasets (ACM (Peeters and Flocon-Cholet 2012), ISMIR04 (Gouyon et al. 2006), Ballroom (BR) (Gouyon et al. 2006), Hainsworth (Hains) (Hainsworth 2004), GTzan (Marchand et al. 2015), SMC (Holzapfel et al. 2012), Giantsteps (GST) (Knees et al. 2015) as well as their union (Combined). Global tempo accuracy and octave errors were evaluated on Acc1 and Acc2 measures, respectively. Performance of several versions of the extended approach was compared against the original Deep Rhythm. A performance increase was observed cross all test datasets, with Acc1 of up to 97.7% and Acc2 of up to 99.4% (both on the GST dataset). Mean improvement across all datasets was 11.2% and 7% for Acc1 and Acc2, respectively. The biggest improvements were apparent for the SMC dataset (16.1% and 23.1% for Acc1 and Acc2, respectively), although performance on this dataset was still sub-par compared to all others. The chief benefits of Aarabi and Peeters’ (2022) DR approach was simultaneous estimation of both tempo and genre of music. While these two aspects are related, it can be seen that AI-based approaches are being developed which can capture increasingly disparate musical features. The overall improvement of several percentage points on most datasets was another clear win for DR. The only real drawback was the sub-par performance on the SMC dataset. Summary and Conclusions Instilling in machines the ability to ‘think and act like humans’ is a defining objective of AI. The author’s goal in this paper has been to survey how AI-based approaches have been utilized to inform understanding of and ability to estimate at scale perception of a fundamental aspect of music, namely its underlying tempo. Given individual variation in how humans define the exact tempo of a piece of music (e.g., at which beat level or octave they tap along to), one way of demonstrating human-like behavior might for a machine to mimic these idiosyncrasies in a convincing manner. Increasing temporal accuracy (Acc0 and Acc1 evaluation) and reducing octave errors (Acc2 evaluation) might thus be considered hallmarks of such behavior. Other indicators might include reduced complexity and increased efficiency. A range of tempo estimation system have been surveyed here that have progressively dealt with these idiosyncrasies and design characteristics. In the process, these algorithms have come to exhibit human-like qualities to an increasingly sophisticated degree. Building on earlier signal processing-oriented methods of tempo estimation based on detection of periodicities in the music, a range of AI-related techniques have been experimented with, including those based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)), classification and statistical learning (e.g., support vector machines (SVM)), and artificial neural networks (ANNs) (e.g., self-organizing maps (SOMs), CNNs, RNNs, deep learning (DL)). Along the way, specific variants of some of these approaches have been developed. These continually improving approaches have, one might argue, steadily encroached upon what is widely regarded a unique human capacity. So how much more accurate can tempo estimation get? In 2019, Schreiber, Urbano and Müller posed the question of whether we’re done yet with tempo estimation. Given the recent, near-perfect results achieved by the likes of Aarabi and Peeters (2022), for instance, which even then leave some small room for improvement, one might argue that the answer to that question is ‘No’. Will we ever be? In light of the myriad peculiarities of the human condition, and despite the attraction and unarguable utility of flawless imitation of human intelligence by a machine, even an ‘intelligent’ one, perfect estimation of perceptual tempo on a genuinely diverse range of music seems likely to remain forever just beyond reach. References Aarabi, H. F., and G. Peeters. 2022. “Extending deep rhythm for tempo and genre estimation using complex convolutions, multitask learning and multi-input network.” Journal of Creative Music Systems. Burger, B., M. R. Thompson, G. Luck, S. Saarikallio, and P. Toiviainen. 2014. “Hunting for the beat in the body: On period and phase locking in music-induced movement.” Frontiers in Human Neuroscience 8(903). Böck, S. 2010. “Onset, beat, and tempo detection with artificial neural networks.” Diploma thesis, Technical University of Munich, Germany, 2010. Böck, S., F. Krebs, and G. Widmer. 2015. “Accurate Tempo Estimation based on Recurrent Neural Networks and Resonating Comb Filters.” In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR), pp 625–631, Málaga, Spain, 2015. Böck, S., F. Krebs, and G. Widmer. 2016. “Joint beat and downbeat tracking with recurrent neural networks,” in International Society for Music Information Retrieval Conference (ISMIR), 2016, pp. 255–261. Böck, S., and M. E. P. Davies. 2020. “Deconstruct, analyse, reconstruct: How to improve tempo, beat, and downbeat estimation.” In Proc. of the 21st Int. Society for Music Information Retrieval Conf., Montréal, Canada, 2020. Böck, S., M. E. P. Davies, and P. Knees. 2019. “Multi-task learning of tempo and beat: Learning one to improve the other.” 20th International Society for Music Information Retrieval Conference, Delft, The Netherlands, 2019. Chen, C.-W., Cremer, M., Lee, K., DiMaria, P., and Wu, H.-H. (2009). Improving perceived tempo estimation by statistical modeling of higher-level musical descriptors. In Audio Engineering Society Convention 126. Audio Engineering Society.  Choi, K., G. Fazekas, and M. Sandler. 2016. “Automatic tagging using deep convolutional neural networks,” In 17th International Society for Music Information Retrieval Conference (ISMIR). Davies, M. E. P., and M.D. Plumbley. 2007. “Context-dependent beat tracking of musical audio.” IEEE Trans. Audio, Speech, Lang. Process., vol. 15(3), pp. 1009–1020. Davies, M. E. P., and S. Böck. 2019. “Temporal convolutional networks for musical audio beat tracking.” In Proc. of the 27th European Signal Processing Conf. (EU- SIPCO), 2019. Drake, C., L. Gros, and A. Penel. 1999. “How fast is that music? The relation between physical and perceived tempo.” In S. W. Yi (Ed.), Music, mind, and science (pp. 190–203), Seoul, Korea: Seoul National University. Dutta, O. 2018. “Tempo octave correction using multiclass support vector machine.” Proceedings of the 2nd International Conference on Inventive Communication and Computational Technologies (ICICCT 2018). \\n\\n------------\\n\\n 8 Eerola, T., and J. K. Vuoskoski. 2013. “A review of music and emotion studies: Approaches, emotion models, and stimuli.” Music Perception 30(3):307–340. Ellis, D. P. 2007. “Beat tracking by dynamic programming.” J. New Music Res., 36(1), pp. 51–60.  Eppler, A., A. Mäncchen, J. Abesser, C. Weiss, and K. Frieler. 2014. “Automatic style classification of jazz recordings with respect to rhythm, tempo, and tonality.” Proceedings of the 9th Conference on Interdisciplinary Musicology – CIM14. Berlin, Germany 2014. Eronen, A. J., and A. P. Klapuri. 2010. “Music Tempo Estimation With k-NN Regression.” IEEE Transactions on Speech and Audio Processing, 18 (1). Faraldo, A., Jorda, S., and Herrera, P. 2017. “A multi-profile method for key estimation in edm.” Proceedings of the AES International Conference on Semantic Audio. Erlangen, Germany: Audio Engineering Society.  Foote, J., M. L. Cooper, and U. Nam. 2002. “Audio retrieval by rhythmic similarity.” In Proc. of ISMIR (International Society for Music Information Retrieval), Paris, France, 2002. Foroughmand, H. and G. Peeters. (2019). “Deep-rhythm for tempo estimation and rhythm pattern recognition.” In Proc. of ISMIR (International Society for Music Information Retrieval), Delft, The Netherlands. Gainza, M., and E. Coyle. 2011. “Tempo detection using a hybrid multiband approach.” IEEE Transactions on Speech and Audio Processing, 19(1), 191-196. Gkiokas, A., V. Katsouros, and G. Carayannis. (2012). Reducing tempo octave errors by periodicity vector coding and svm learning. In Proc. of ISMIR (International Society for Music Information Retrieval), Porto, Portugal.  Gkiokas, A., V. Katsouros, G. Carayannis, and T. Stafylakis. 2012. “Music tempo estimation and beat tracking by applying source separation and metrical relations.” in Proc. Int. Conf. Acoust., Speech, Signal Process., pp. 421-424. Goodchild, M., B. Gingras, and S. McAdams. 2016. “Analysis, performance, and tension perception of an unmeasured prelude for harpsichord.” Music Perception 34(1):1–20. Goto, M. 2001. “An audio-based real-time beat tracking system for music with or without drum-sounds.” Journal of New Music Research,  pp. 159–171. Gouyon, F., A. P. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis, C. Uhle, and P. Cano. 2006. “An experimental comparison of audio tempo induction algorithms.“ IEEE Transactions on Audio, Speech, and Language Processing, 14(5):1832– 1844. 2006.  Grzywczak, D., and Gwardys, G. 2014. “Audio features in music information retrieval.” In D. Slezak, G. Schaefer, S. T. Vuong, and Y-S. Kim (Eds.) Proceedings of Active Media Technology, 10th International Conference, Warsaw, Poland, August 11–14, 2014. Hainsworth, S. W. 2004. “Techniques for the automated analysis of musical audio.” PhD thesis, University of Cambridge, UK, September 2004. Han, Y., J. Kim, and K. Lee. 2016. “Deep convolutional neural networks for predominant instrument recognition in polyphonic music.” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25(1), 208-221. Holzapfel, A., and Y. Stylianou. 2011. “Scale transform in rhythmic similarity of music.” IEEE Transactions on Audio, Speech and Language Processing, 19(1):176–185, 2011.  Holzapfel, A., M. E. P. Davies, J. R. Zapata, J. L. Oliveira, and F. Gouyon. 2012. “Selective sampling for beat tracking evaluation.” IEEE Transactions on Audio, Speech, and Language Processing, 20(9), 2539–2548, 2012.  Hörschläger, F., R. Vogl, S. Böck, and P. Knees. 2015. “Addressing tempo estimation octave errors in electronic music by incorporating style information extracted from Wikipedia”, In Proc. of the Sound and Music Computing Conference (SMC), Maynooth, Ireland, 2015. Istvanek, M. 2021. “The application of tempo calculation for musicological purposes.” Doctoral Degree Programme (2), FEEC BUT. DOI: 10.13164/eeict.2021.265 Klapuri, A., A. J. Eronen, and J. T. Astola. 2006. “Analysis of meter of acoustic music signals.” IEEE transactions on audio, speech, and language processing, 14(1), 342–355. Knees, P., Á. Faraldo, P. Herrera, R. Vogl, S. Böck, F. Hörschläger, and M. Le Goff. 2015. “Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections.” In Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR), pp 364–370, Málaga, Spain, October 2015.  Kraft, S., A.Lerch, and U. Zölzer. 2013. “The tonalness spectrum: feature-based estimation of tonal components.” Proceedings of the 16th International Conference on Digital Audio Effects (DAFx), Maynooth, Ireland. Lartillot, O., and P. Toiviainen. 2007. “A Matlab toolbox for musical feature extraction from audio.” In Proceedings of the 10th International Conference on Digital Audio Effects, pp. 237-244. Levy, M. (2011). Improving perceptual tempo estimation with crowd-sourced annotations. In Proc. of ISMIR (International Society for Music Information Retrieval), Miami, Florida, USA.  Li, T. LH., and A. B. Chan. 2011. “Genre classification and the invariance of MFCC features to key and tempo.” In International Conference on MultiMedia Modeling pp. 317–327. Marchand, U., and G. Peeters. 2014. “The modulation scale spectrum and its application to rhythm-content description.” In Proc. of DAFx (International Conference on Digital Audio Effects), Erlangen, Germany, 2014.  Marchand, U., Fresnel, Q., and Peeters, G. (2015). “GTZAN-Rhythm: Extending the GTZAN Test-Set with Beat, Downbeat and Swing Annotations.” Late-Breaking Demo Session of the 16th International Society for Music Information Retrieval Conference, 2015.  Marchand, U., and G. Peeters. 2016. “Scale and shift invariant time/frequency representation using auditory statistics: Application to rhythm description.” In 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2016.  Marchand, U., and G. Peeters. 2016 b. “The extended ballroom dataset.” Late Breaking Demo of the International Conference on Music Information Retrieval (ISMIR), New York, NY, USA. Müller M., V. Konz, A. Scharfstein, S. Ewert, and M. Clausen. 2009. “Towards automated extraction of tempo parameters from expressive music recordings.\" In: k. Hirata, G. Tzanetakis, K. Yoshii K (Eds.) ISMIR 2009, 10th international conference on music information retrieval. Oliveira, J. L., F. Gouyon, L. G. Martins, and L. P. Reis. 2010. “IBT: A real-time tempo and beat tracking system,” in Proc. Int. Soc. For Music Information Retrieval (ISMIR), pp. 291– 296, 2010. Oyama, T., R. Ishizuka, and K. Yoshii. 2021. “Phase-aware joint beat and downbeat estimation based on periodicity of metrical structure.” in Proc. of the 22nd Int. Society for Music Information Retrieval Conf., Online, 2021. Peeters. G. 2011. “Spectral and temporal periodicity representations of rhythm for the automatic classification of music audio signal.” IEEE Transactions on Audio, Speech and Language Processing, 19(5):1242– 1252, 2011.  Peeters, G., and J. Flocon-Cholet. 2012. “Perceptual tempo estimation using GMM-regression.” In Proceedings of the second international ACM workshop on Music information retrieval with user-centered and multimodal strategies (MIRUM), pp 45–50, New York, NY, USA, 2012. ACM.  Percival, G. and Tzanetakis, G. (2014). “Streamlined tempo estimation based on autocorrelation and cross-correlation with pulses.” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(12):1765–1776.  Phan, H., L. Hertel, M. Maass, and A. Mertins. 2016. “Robust audio event recognition with 1-max pooling convolutional neural networks.” arXiv preprint arXiv:1604.06338. Pons, J., T. Lidy, and X. Serra. 2016. “Experimenting with musically motivated convolutional neural networks.” In 14th International Workshop on Content- Based Multimedia Indexing (CBMI). IEEE. Pons, J., and X. Serra. 2017. “Designing efficient architectures for modelling temporal features with convolutional neural networks.” In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 2472-2476). IEEE. Raffel, C. (2016). Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching. PhD thesis, Columbia University.  Sachs, C. 1953. Rhythm and Tempo. London: Dent. Scheirer, E. D. 1998. “Tempo and beat analysis of acoustic musical signals.” J Acoust. Soc. Amer., 103(1), 588–601. \\u2028  Schlüter, J., and S. Böck. 2014. “Improved musical onset detection with convolutional neural networks.” In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. Schreiber. H. 2017. “CNN-based automatic musical key detection.” Music Information Retrieval Evaluation eXchange (MIREX), Suzhou, China. Schreiber, H. and Müller, M. (2017). A post-processing procedure for improving music tempo estimates using supervised learning. In Proc. of ISMIR (International Society for Music Information Retrieval), Suzhou, China.  \\n\\n------------\\n\\n 9 Schreiber, H., and M. Müller. 2018. “A single-step approach to musical tempo estimation using a convolutional neural network.” In 19th International Society for Music Information Retrieval Conference, Paris, France, 2018. Schreiber, H., and M. Müller. 2018 b. “A crowdsourced experiment for tempo estimation of electronic dance music.” Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR), Paris, France, September 2018. Schreiber, H., and M. Müller. 2019 a. “Musical tempo and key estimation using convolutional neural networks with directional filters.” arXiv preprint arXiv:1903.10839. Schreiber, H., J. Urbano, and M. Müller. 2019 b. Music tempo estimation: Are we done yet? Transactions of the International Society for Music Information Retrieval, 3(1), 111–125. Schreiber, H., F. Zalkow, and M. Müller. 2020. “Modeling and estimating local tempo: A case study on Chopin’s Mazurkas.” In Proc. of the 21st Int. Society for Music Information Retrieval Conf., Montréal, Canada, 2020.   Seyerlehner, K., Widmer, G., and Schnitzer, D. (2007). From rhythm patterns to perceived tempo. In Proc. of ISMIR (International Society for Music Information Retrieval), Vienna, Austria.  Tzanetakis, G., and P. Cook. 2002. “Musical genre classification of audio signals.” IEEE Transactions on Speech and Audio Processing, 10(5):293–302, 2002. van den Oord, A., S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. 2016. “Wavenet: A generative model for raw audio.” arXiv preprint arXiv:1609.03499. Webster, G. D. and C. G. Weir. 2005. “Emotional responses to music: Interactive effects of mode, texture, and tempo.” Motivation and Emotion 29(1):19-39. Wu, F-H. F. 2015. “Music tempo octave error reducing based on the statistics of tempogram.” 23rd Mediterranean Conference on Control and Automation (MED) June 16-19, 2015. Torremolinos, Spain. Wu, F-H. F. and J-S. R. Jang. 2014. “A supervised learning method for tempo estimation of musical audio”, In 22nd Mediterranean Conference of Control and Automation (MED), 2014, 599–604. IEEE, 2014. Wu, F.-H. F., T-C. Lee, J-S. R. Jang, K. K. Chang, C.-H. Lu, and W-N. Wang, 2011. “A two-fold dynamic programming approach to beat tracking for audio music with time-varying tempo.” in International Society for Music Information Retrieval Conference (ISMIR), 2011, pp. 191–196. Xiao, L., Tian, A., Li, W., and Zhou, J. (2008). “Using statistic model to capture the association between timbre and perceived tempo.” In Proc. of ISMIR (International Society for Music Information Retrieval), Philadelphia, PA, USA. Zapata, J. R., and Gómez, E. 2011. “Comparative evaluation and combination of audio tempo estimation approaches.” In Audio engineering society conference: 42nd international conference: Semantic audio. Audio Engineering Society.  i https://pypi.org/project/madmom/ '"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store['AI and Tempo Estimation.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FileBytesPersister with relative paths'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save_here.json',\n",
       " 'nested.json',\n",
       " 'AI and Tempo Estimation.pdf',\n",
       " 'JAMMIN-GPT.pdf',\n",
       " 'simple.docx',\n",
       " 'Release Notes.docx',\n",
       " 'simple.txt']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You could also apply extension_base_wrap to a class\n",
    "\n",
    "@extension_base_wrap\n",
    "class MySpecialFiles(Files):\n",
    "    \"\"\"Decodes files according to some specific extension rules\"\"\"\n",
    "\n",
    "store2 = MySpecialFiles(misc_files_path)\n",
    "list(store2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Latest Release\\nVersion: v0.0.20\\nFeatures/changes:\\nAdded data storage management feature, which will delete uploaded blocks, if memory usage is more than 90 and also, it will show alerts to users.\\nAdded more streaming parameters configurable through CLI, they are,\\nBuffer duration in seconds\\nBlock duration in seconds\\nChunk duration in seconds\\nCheck the help in CLI for more information on new parameters\\nAdded Amplification feature in oedge. \\nUsers can see the current amplification value on get-config. \\nAlso can update it on set-config \\n\\nPrevious Releases\\n\\nVersion: v0.0.19\\nFeatures/changes:\\nChange in Data storage service hosted VM IP\\n\\nVersion: v0.0.18\\nFeatures/changes:\\nUpload Service is now working on periodic trigger instead of Redis Trigger \\n\\n\\nVersion: v0.0.17\\nFeatures/changes:\\nIncluded new version of AudioStream2py(0.1.21) library with bug fixes\\nAdded Session ID details to CLI when starting a capture\\n\\nVersion: v0.0.16\\nFeatures/changes:\\nIncluded new version of AudioStream2py(0.1.20) library with bug fixes\\nFixed internal issue occurring when user starts a new session just after completing a session\\nCreated validation for not allowing users to create a new session when another session is already running \\n\\nVersion: v0.0.15\\nFeatures/changes:\\nRemoved chunk_size filed and added chunk_duration field in SS configuration file\\nAdded logs on CLI when DSS is down\\nChanged CLI behavior as the user needs to wait till a block duration after pressed ctrl+c\\nUpdated Setup document with the steps for restarting DSS\\n\\nVersion: v0.0.14\\nFeatures/changes:\\nFixed DPP annotation upload issue occurring due to data size limitation in Cosmos DB\\nChanged logger timezone to UTC\\n\\nVersion: v0.0.13\\nFeatures/changes:\\nImproved logs on CLI on timer expiry and user stopping cases\\nAdded separate log files for message queue, services and CLI,\\nMessage Queue -> msg_queue_oedge_0.0.13.log\\nCLI -> cli_oedge_0.0.13.log\\nSS -> stream_service_oedge_0.0.13.log\\nLSS -> local_storage_service_oedge_0.0.13.log\\nUS -> upload_service_oedge_0.0.13.log\\n\\nVersion: v0.0.12\\nFeatures/changes:\\nModified US to handle create session failure\\nModified US and LSS such that all messages go on single channel \\nFixed issue on CLI getting stuck on sessions with lesser duration\\n\\nVersion: v0.0.11\\nFeatures/changes:\\nAdded logs for LSS, SRS &  AS.\\nAdded block count in logs.\\nFix for start-capture with -dpp option with and without timer.\\n\\nVersion: v0.0.10\\nFeatures/changes:\\nDPP installation fix\\nBetter DSS logs\\n\\nVersion: v0.0.9\\nFeatures/changes:\\nAdded below logs:\\nIn SRS SDI result.\\nCLI: Each user action and other info for debugging.\\nAdded log rotation feature with maximum file size 5 MB and maximum file count 50(Total 250 Mb) per services\\nAll service log files are combined in to single log file: oedge_0.0.9.log\\nLog files path changed to: /app/logs\\nLog files can be zipped and copied to the host machine. Please refer “How to Get logs of Oedge” section in setup document\\nConfig files path changed to: /app/configs\\n\\n\\nVersion: v0.0.8\\nFeatures/changes:\\nAdded more logs for all the services. This can be used for analyzing gap issue in recordings\\nhttps://app.asana.com/0/1203916667080587/1204747069758805\\nLog file names changed to service related names as follows,\\nStream Service -> stream_service.log\\nLocal Storage Service -> local_storage_service.log\\nUpload Service -> upload_service.log\\nAdded more logs to DSS server, to find the issue regarding accessibility\\n\\nVersion: v0.0.7\\nFeatures/changes:\\nFixed Gap issue in session audio file\\nFixed issue, where CLI cannot stop session on pressing Ctrl + C at some edge case\\nAdded new logs for all three services for specific case checks\\nMade DSS running continuously on host, so that QA hasn’t need to take care of it\\n\\nVersion: v0.0.6\\nFeatures/changes:\\nRemoved autoselection of input device. Now the user needs to update the input device name in the config file. Steps for the same are explained in the Setup Document. By default, it will choose IEPE converters.\\nPersisted the Configuration files in the host system. So, even if we rebuilt or deleted the container, the config changes will remain.\\nIntegrated new data storage service from platform_poc with the device\\nConfig updation on stream service config file where done by CLI previously. Now this is changed and done by stream service itself.\\nSession file saving path has been changed. See Verify the Session file section in the Setup Document.\\nAzure MongoDB collection name changed from session_data_collection to “sessions”\\n\\n\\n\\n\\nVersion: v0.0.5\\nFeatures/changes:\\nRenamed services names as,\\nredis -> oedge_redis\\nmongo -> oedge_mongodb\\nother_services -> oedge_app\\n\\nVersion: v0.0.4\\nFeatures/changes:\\nUsed slabs in Analytics Service & Session Storage Service\\nChanged service names \\nRemoved audio device index from config\\nStreamService will check for IEPE, if not present it will use system default. \\nIf system default is not present or audio input does not work, StreamService will throw an error and exit. \\n\\nVersion: v0.0.3\\nFeatures:\\nAdded DPP annotations to Local and Azure MongoDB\\nAdded Autoselect IEPE converter feature\\n\\n\\nVersion: v0.0.2\\nFeatures:\\nReplaced .wav files with .pcm files\\nAdded DPP download option\\nCan get dummy score from DPP if DPP enabled while starting capture\\n\\nVersion: v0.0.1\\nFeatures:\\nImplemented end to end flow from CLI to MongoDB\\nFlow\\nCLI -> Stream Service(Session Recording Service(SRS), Analytic Service(AS)) -> Local Storage Service -> Upload Service\\nCLI\\nCLI used to interact with DAQ\\nStream Service\\nCreates two buffer readers for SRS and AS\\nSRS will create blocks from buffers and sent to local storage service\\nAS will create blocks from buffers and analyze it using DPP and sent a score to CLI\\nLocal Storage Service\\nSave the block as a wav file locally and save session details in Mongodb. Also forward the data to Upload service\\nUpload Service\\nSave block on Azure file storage and CosmoDb using Data Storage API’s\\nData Storage Service\\nRunning on Azure VM.\\nCLI is implemented as per following document\\nhttps://docs.google.com/document/d/11Fiuu-3gx6DMeL4tYVsB9GHDAUEZsvra/edit?usp=share_link&ouid=113315534903626710685&rtpof=true&sd=true\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store2['Release Notes.docx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfdol.base import bytes_to_pdf_reader_obj, read_pdf_text\n",
    "from msword import bytes_to_doc, get_text_from_docx\n",
    "\n",
    "extension_to_decoder = {\n",
    "    '.txt': lambda obj: obj.decode('utf-8'),\n",
    "    '.json': json.loads,\n",
    "    '.pdf': Pipe(\n",
    "        bytes_to_pdf_reader_obj, read_pdf_text, '\\n\\n------------\\n\\n'.join\n",
    "    ),\n",
    "    '.docx': Pipe(bytes_to_doc, get_text_from_docx),\n",
    "}\n",
    "\n",
    "from dol import wrap_kvs, Pipe\n",
    "\n",
    "def extension_based_decoding(k, v):\n",
    "    ext = '.' + k.split('.')[-1]\n",
    "    decoder = extension_to_decoder.get(ext, None)\n",
    "    if decoder is None:\n",
    "        raise ValueError(f\"Unknown extension: {ext}\")\n",
    "    return decoder(v)\n",
    "\n",
    "def extension_base_wrap(store):\n",
    "    return wrap_kvs(store, postget=extension_based_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfdol.base import bytes_to_pdf_reader_obj, read_pdf_text\n",
    "from msword import bytes_to_doc, get_text_from_docx\n",
    "\n",
    "extension_to_decoder = {\n",
    "    '.txt': lambda obj: obj.decode('utf-8'),\n",
    "    '.json': json.loads,\n",
    "    '.pdf': Pipe(\n",
    "        bytes_to_pdf_reader_obj, read_pdf_text, '\\n\\n------------\\n\\n'.join\n",
    "    ),\n",
    "    '.docx': Pipe(bytes_to_doc, get_text_from_docx),\n",
    "}\n",
    "\n",
    "from dol import wrap_kvs, Pipe\n",
    "\n",
    "def mk_extension_based_decoding(extension_to_decoder):\n",
    "    def extension_based_decoding(k, v):\n",
    "        ext = '.' + k.split('.')[-1]\n",
    "        decoder = extension_to_decoder.get(ext, None)\n",
    "        if decoder is None:\n",
    "            raise ValueError(f\"Unknown extension: {ext}\")\n",
    "        return decoder(v)\n",
    "    return extension_based_decoding\n",
    "\n",
    "\n",
    "def extension_base_wrap(store, extension_to_decoder):\n",
    "    extension_based_decoding = mk_extension_based_decoding(extension_to_decoder)\n",
    "    return wrap_kvs(store, postget=extension_based_decoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "dflt_extension_to_decoder = {\n",
    "    '.txt': lambda obj: obj.decode('utf-8'),\n",
    "    '.json': json.loads,\n",
    "    '.pdf': Pipe(\n",
    "        bytes_to_pdf_reader_obj, read_pdf_text, '\\n\\n------------\\n\\n'.join\n",
    "    ),\n",
    "    '.docx': Pipe(bytes_to_doc, get_text_from_docx),\n",
    "}\n",
    "\n",
    "def extension_based_decoding(k, v, *, extension_to_decoder=dflt_extension_to_decoder):\n",
    "    ext = '.' + k.split('.')[-1]\n",
    "    decoder = extension_to_decoder.get(ext, None)\n",
    "    if decoder is None:\n",
    "        raise ValueError(f\"Unknown extension: {ext}\")\n",
    "    return decoder(v)\n",
    "\n",
    "\n",
    "def extension_base_wrap(store, *, extension_to_decoder=dflt_extension_to_decoder):\n",
    "    _extension_based_decoding = partial(extension_based_decoding, extension_to_decoder=extension_to_decoder)\n",
    "    return wrap_kvs(store, postget=_extension_based_decoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save_here.json',\n",
       " 'nested.json',\n",
       " 'AI and Tempo Estimation.pdf',\n",
       " 'JAMMIN-GPT.pdf',\n",
       " 'simple.docx',\n",
       " 'Release Notes.docx',\n",
       " 'simple.txt']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_and_log(v):\n",
    "    print(f\"Decoding: {v}\")\n",
    "    return v.decode()\n",
    "\n",
    "store = extension_base_wrap(s, extension_to_decoder={'.txt': decode_and_log})\n",
    "list(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding: b'This is\\nJust some text\\nTo test things'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is\\nJust some text\\nTo test things'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store['simple.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
